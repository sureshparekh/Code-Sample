{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with PyTorch\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, we will introduce you to PyTorch, a highly-regarded open-source deep learning framework developed by Facebook AI Research. Renowned for its flexibility, user-friendly nature, and dynamic computational graph, PyTorch facilitates effortless debugging and experimentation for developers working with deep learning models. For additional reading and resources, visit the official PyTorch [website](https://pytorch.org/) and the PyTorch GitHub [repository](https://github.com/pytorch/pytorch).\n",
    "\n",
    "Our exploration will begin with an examination of PyTorch's fundamental features, followed by a demonstration of how to create and manipulate Tensor objects â€“ the core components of PyTorch. We will also cover a variety of algebraic operations, including matrix multiplication and elementwise multiplication, which play a vital role in constructing and training deep learning models. To further your understanding, consult the [PyTorch documentation](https://pytorch.org/docs/stable/index.html).\n",
    "\n",
    "Next, we will dive into one of PyTorch's most powerful features, Autodifferentiation, also known as Autograd. This remarkable capability automates the process of calculating gradients of a neural network's parameters in relation to the loss function. In conjunction with Stochastic Gradient Descent and Backpropagation, Autograd is instrumental in training large machine learning models, enabling more efficient and precise model optimization. To learn more about Autograd, explore the [PyTorch Autograd tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html).\n",
    "\n",
    "As we progress through this guide, we will also touch on the topic of utilizing a Graphics Processing Unit (GPU) with PyTorch. This integration can significantly accelerate the training process for large-scale models. To gain further insights into using GPUs with PyTorch, review the \"CUDA semantics\" section in the [PyTorch documentation](https://pytorch.org/docs/stable/notes/cuda.html).\n",
    "\n",
    "Without further ado, let's ignite our journey into the world of PyTorch, and remember to explore the resources provided above for an even deeper understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To install PyTorch, you can use either pip or conda, depending on your preferred package manager. Before installing, it's important to determine whether you need a version that supports CPU only or one that supports both CPU and GPU (CUDA).\n",
    "\n",
    "To choose the appropriate installation command, visit the PyTorch official website's [\"Get Started\" page](https://pytorch.org/get-started/locally/). You'll find a configuration widget that lets you select your preferred package manager, OS, Python version, and CUDA version (if needed).\n",
    "\n",
    "After the installation is complete, you can verify that PyTorch has been successfully installed by running the following commands in your Python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also import other useful packages for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suresh/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/suresh/.local/lib/python3.8/site-packages/pandas/core/arrays/masked.py:64: UserWarning: Pandas requires version '1.3.2' or newer of 'bottleneck' (version '1.2.1' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# display matplotlib plots inline in the Jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Import necessary packages\n",
    "import torch  # deep learning library\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib  # plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  # data manipulation library\n",
    "import seaborn as sns  # data visualization library\n",
    "from cycler import cycler\n",
    "from IPython.display import Video, Image  # display videos and images in Jupyter notebook\n",
    "\n",
    "# Define the value of Pi for pytorch as it doesn't have a pre-defined value\n",
    "torch_pi = 3.1415916\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Define some properties for plotting\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# Define a function to convert RGB values to matplotlib color\n",
    "def rgb(r, g, b):\n",
    "    return (float(r)/256., float(g)/256., float(b)/256.)\n",
    "\n",
    "# Define the color cycle for plots\n",
    "cb2 = [rgb(31, 120, 180), rgb(255, 127, 0), rgb(51, 160, 44), rgb(227, 26, 28),\n",
    "       rgb(166, 206, 227), rgb(253, 191, 111), rgb(178, 223, 138), rgb(251, 154, 153)]\n",
    "\n",
    "# Set some properties for figures and plots\n",
    "rcParams['figure.figsize'] = (9, 7)  # figure size\n",
    "rcParams['figure.dpi'] = 50  # figure resolution\n",
    "rcParams['lines.linewidth'] = 2  # line width\n",
    "rcParams['axes.prop_cycle'] = cycler('color', cb2)  # color cycle\n",
    "rcParams['axes.facecolor'] = 'white'  # background color\n",
    "rcParams['axes.grid'] = False  # display grid or not\n",
    "rcParams['patch.facecolor'] = cb2[0]  # color for patches\n",
    "rcParams['patch.edgecolor'] = 'white'  # color for patch edges\n",
    "rcParams['font.size'] = 23  # font size\n",
    "rcParams['font.weight'] = 300  # font weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Tensor and PyTorch \n",
    "\n",
    "PyTorch predominantly utilizes tensors as its core data structure to encode various types of information, such as model inputs and outputs, along with model parameters. Tensors are multi-dimensional arrays that extend scalars, vectors, and matrices to encompass data in $N$-dimensions, allowing for the representation of diverse data structures from simple to highly complex forms.\n",
    "\n",
    "To clarify further:\n",
    "\n",
    "- Scalar (0-dimensional tensor): A single value, e.g., 1.0\n",
    "\n",
    "- Vector (1-dimensional tensor): A one-dimensional array, e.g.,\n",
    "$\\begin{bmatrix}\n",
    "1 \\\\\n",
    "2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- Matrix (2-dimensional tensor): A two-dimensional array, e.g.,\n",
    "$\\begin{bmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- Tensor (n-dimensional tensor, $n > 2$): An N-dimensional array, e.g.,\n",
    "$\\begin{bmatrix}\n",
    "[1, 2] & [3, 4] \\\\\n",
    "[4, 5] & [6, 7]\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Tensors provide a flexible way to represent data across a broad range of dimensions, making them particularly valuable in deep learning and other intricate mathematical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch and Numpy\n",
    "\n",
    "We will first explore the relationship between PyTorch and NumPy, two widely-used libraries for numerical computing in Python. While PyTorch is specifically designed for deep learning applications, NumPy is a general-purpose library for numerical operations. Despite their distinct purposes, PyTorch aims to maintain a similar API to NumPy, enabling users to transition effortlessly between the two libraries.\n",
    "\n",
    "Moreover, we will demonstrate how to convert data between PyTorch tensors and NumPy arrays, which is beneficial when working with both libraries simultaneously in your projects.\n",
    "\n",
    "It's crucial to note that when converting between PyTorch tensors and NumPy arrays, the underlying data is shared between them if the tensor is on the CPU. As a result, modifying one will impact the other. This design choice aids in conserving memory and minimizing the overhead of data copying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting a NumPy array to a PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original NumPy array:\n",
      " [[1 2]\n",
      " [3 4]]\n",
      "\n",
      "PyTorch tensor from NumPy array:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "Modified NumPy array:\n",
      " [[99  2]\n",
      " [ 3  4]]\n",
      "\n",
      "PyTorch tensor after modifying NumPy array:\n",
      " tensor([[99,  2],\n",
      "        [ 3,  4]])\n"
     ]
    }
   ],
   "source": [
    "# Create a NumPy array\n",
    "numpy_array = np.array([[1, 2], [3, 4]])\n",
    "print(\"Original NumPy array:\\n\", numpy_array)\n",
    "print(\"\")\n",
    "\n",
    "# Convert NumPy array to PyTorch tensor\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(\"PyTorch tensor from NumPy array:\\n\", tensor_from_numpy)\n",
    "print(\"\")\n",
    "\n",
    "# Modify NumPy array\n",
    "numpy_array[0, 0] = 99\n",
    "print(\"Modified NumPy array:\\n\", numpy_array)\n",
    "print(\"\")\n",
    "\n",
    "# Observe the change in the PyTorch tensor\n",
    "print(\"PyTorch tensor after modifying NumPy array:\\n\", tensor_from_numpy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting a PyTorch tensor to a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original PyTorch tensor:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "NumPy array from PyTorch tensor:\n",
      " [[1 2]\n",
      " [3 4]]\n",
      "\n",
      "Modified PyTorch tensor:\n",
      " tensor([[99,  2],\n",
      "        [ 3,  4]])\n",
      "\n",
      "NumPy array after modifying PyTorch tensor:\n",
      " [[99  2]\n",
      " [ 3  4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch tensor\n",
    "tensor = torch.tensor([[1, 2], [3, 4]])\n",
    "print(\"Original PyTorch tensor:\\n\", tensor)\n",
    "print(\"\")\n",
    "\n",
    "# Convert PyTorch tensor to NumPy array\n",
    "numpy_array_from_tensor = tensor.numpy()\n",
    "print(\"NumPy array from PyTorch tensor:\\n\", numpy_array_from_tensor)\n",
    "print(\"\")\n",
    "\n",
    "# Modify PyTorch tensor\n",
    "tensor[0, 0] = 99\n",
    "print(\"Modified PyTorch tensor:\\n\", tensor)\n",
    "print(\"\")\n",
    "\n",
    "# Observe the change in the Numpy array\n",
    "print(\"NumPy array after modifying PyTorch tensor:\\n\", numpy_array_from_tensor)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We will now delve into various PyTorch tensor operations. Many of these operations are intentionally designed to resemble their NumPy counterparts, making it easier for developers to transition between the two libraries seamlessly.\n",
    "\n",
    "### Tensor Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor:\n",
      " tensor([[0.2158, 0.8096, 0.1743],\n",
      "        [0.2995, 0.5971, 0.6132]])\n",
      "\n",
      "Ones tensor:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Zeros tensor:\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Arange tensor:\n",
      " tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Create a random tensor of size (2, 3)\n",
    "rand_tensor = torch.rand(2, 3)\n",
    "print(\"Random tensor:\\n\", rand_tensor)\n",
    "print(\"\")\n",
    "\n",
    "# Create a tensor filled with ones, of size (2, 3)\n",
    "ones_tensor = torch.ones(2, 3)\n",
    "print(\"Ones tensor:\\n\", ones_tensor)\n",
    "print(\"\")\n",
    "\n",
    "# Create a tensor filled with zeros, of size (2, 3)\n",
    "zeros_tensor = torch.zeros(2, 3)\n",
    "print(\"Zeros tensor:\\n\", zeros_tensor)\n",
    "print(\"\")\n",
    "\n",
    "# Create a tensor with a range of values from 0 to 4\n",
    "arange_tensor = torch.arange(5)\n",
    "print(\"Arange tensor:\\n\", arange_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated tensor:\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate tensors along a dimension (dim=0 by default)\n",
    "tensor_a = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor_b = torch.tensor([[5, 6], [7, 8]])\n",
    "concatenated_tensor = torch.cat((tensor_a, tensor_b))\n",
    "print(\"Concatenated tensor:\\n\", concatenated_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed tensor:\n",
      " tensor([[1, 3],\n",
      "        [2, 4]])\n",
      "\n",
      "One-like tensor:\n",
      " tensor([[1, 1],\n",
      "        [1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# Transpose a tensor\n",
    "transposed_tensor = tensor_a.T\n",
    "print(\"Transposed tensor:\\n\", transposed_tensor)\n",
    "print(\"\")\n",
    "\n",
    "# Create a tensor with the same shape but filled with ones\n",
    "one_like_tensor = torch.ones_like(tensor_a)\n",
    "print(\"One-like tensor:\\n\", one_like_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Mathematical Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum:\n",
      " tensor(10)\n",
      "\n",
      "Mean:\n",
      " tensor(2.5000)\n",
      "\n",
      "Norm:\n",
      " tensor(5.4772)\n",
      "\n",
      "Absolute values:\n",
      " tensor([[0, 1],\n",
      "        [2, 3]])\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sum of a tensor's elements\n",
    "sum_result = torch.sum(tensor_a)\n",
    "print(\"Sum:\\n\", sum_result)\n",
    "print(\"\")\n",
    "\n",
    "# Calculate the mean of a tensor's elements\n",
    "mean_result = torch.mean(tensor_a.float())  # Requires float tensor\n",
    "print(\"Mean:\\n\", mean_result)\n",
    "print(\"\")\n",
    "\n",
    "# Calculate the L2-norm of a tensor\n",
    "norm_result = torch.norm(tensor_a.float())  # Requires float tensor\n",
    "print(\"Norm:\\n\", norm_result)\n",
    "print(\"\")\n",
    "\n",
    "# Calculate the absolute values of a tensor\n",
    "abs_result = torch.abs(tensor_b - 5)\n",
    "print(\"Absolute values:\\n\", abs_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition:\n",
      " tensor([5, 7, 9])\n",
      "\n",
      "Subtraction:\n",
      " tensor([-3, -3, -3])\n",
      "\n",
      "Multiplication:\n",
      " tensor([ 4, 10, 18])\n",
      "\n",
      "Division:\n",
      " tensor([0.2500, 0.4000, 0.5000])\n",
      "\n",
      "Matrix multiplication:\n",
      " tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "\n",
      "Dot product:\n",
      " tensor(32)\n"
     ]
    }
   ],
   "source": [
    "# Tensors\n",
    "tensor_a = torch.tensor([1, 2, 3])\n",
    "tensor_b = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Element-wise addition\n",
    "add_result = tensor_a + tensor_b\n",
    "\n",
    "# Element-wise subtraction\n",
    "sub_result = tensor_a - tensor_b\n",
    "\n",
    "# Element-wise multiplication\n",
    "mul_result = tensor_a * tensor_b\n",
    "\n",
    "# Element-wise division\n",
    "div_result = tensor_a / tensor_b\n",
    "\n",
    "# Print element-wise results\n",
    "print(\"Addition:\\n\", add_result)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Subtraction:\\n\", sub_result)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Multiplication:\\n\", mul_result)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Division:\\n\", div_result)\n",
    "print(\"\")\n",
    "\n",
    "# Matrix multiplication\n",
    "mat_a = torch.tensor([[1, 2], [3, 4]])\n",
    "mat_b = torch.tensor([[5, 6], [7, 8]])\n",
    "matmul_result = mat_a@mat_b\n",
    "print(\"Matrix multiplication:\\n\", matmul_result)\n",
    "print(\"\")\n",
    "\n",
    "# Dot product\n",
    "dot_result = torch.dot(tensor_a, tensor_b)\n",
    "print(\"Dot product:\\n\", dot_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Data Types in PyTorch\n",
    "\n",
    "In this section of the tutorial, we will concentrate on the two main floating-point data types available in PyTorch: 32-bit floats (float32) and 64-bit floats (float64). Gaining a solid understanding of these data types and their characteristics is vital when working with tensors effectively, particularly when harnessing the power of GPU capabilities.\n",
    "\n",
    "Float32, or single-precision floating-point numbers, are the most widely used data type in PyTorch for representing real numbers with decimal points. They strike a good balance between precision and memory consumption, making them suitable for the majority of deep learning applications. Most GPUs natively support float32, resulting in quicker computation. However, float32's precision is limited compared to float64, which may lead to numerical issues in certain situations.\n",
    "\n",
    "Float64, or double-precision floating-point numbers, provide a higher level of precision than float32. However, this increased precision comes at the expense of greater memory consumption and slower computation, particularly on GPUs.\n",
    "\n",
    "GPUs are optimized for parallel processing, which is highly advantageous for deep learning applications. Native GPU support for float32 is standard because it offers an optimal balance between precision and memory usage. Consequently, most deep learning frameworks, including PyTorch, default to using float32 for tensor operations, especially when executed on GPUs. This approach enables faster training and inference times while maintaining sufficient precision for the majority of use cases.\n",
    "\n",
    "To specify the data type of a tensor, you can use the dtype argument when creating the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "# Create a float32 tensor\n",
    "float32_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "print(float32_tensor.dtype)\n",
    "\n",
    "# Create a float64 tensor\n",
    "float64_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)\n",
    "print(float64_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the various data types in PyTorch and their implications for computation and memory usage is crucial for building efficient deep learning models and leveraging GPU capabilities to their fullest extent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Optimisation\n",
    "\n",
    "In this section, we will explore how to utilize GPUs in PyTorch, specifically in the context of Google Colab. Google Colab provides free access to a GPU for accelerating deep learning tasks. We will discuss how to check if a GPU is being used, and how to transfer data between CPU and GPU memory.\n",
    "\n",
    "Google Colab provides an environment where you can easily enable GPU support for your deep learning projects. To do this, navigate to the \"Runtime\" menu, select \"Change runtime type,\" and choose \"GPU\" as the hardware accelerator. With this setup, you can leverage the power of GPUs for your PyTorch projects. You will also see below that we will refer GPU as CUDA.\n",
    "\n",
    "CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA for general-purpose computing on GPUs (Graphics Processing Units). CUDA allows developers to write programs that can take advantage of the massive parallel processing power of NVIDIA GPUs, which are particularly well-suited for tasks that involve a high degree of parallelism, such as deep learning and other scientific computations.\n",
    "\n",
    "In the context of deep learning, CUDA plays a significant role in speeding up the training and inference processes by utilizing the parallel processing capabilities of NVIDIA GPUs. These GPUs are designed to handle thousands of lightweight threads simultaneously, which is a key factor in the efficient execution of deep learning algorithms that involve large amounts of matrix and tensor operations.\n",
    "\n",
    "To use CUDA in deep learning frameworks like PyTorch, NVIDIA also provides cuDNN (CUDA Deep Neural Network library), which is a GPU-accelerated library for deep learning primitives. cuDNN optimizes the performance of standard deep learning routines, such as forward and backward propagation, convolution, pooling, normalization, and activation layers. All these are automatically included when you run Pytorch on GPU.\n",
    "\n",
    "To check if a GPU is being used by PyTorch, you can use the torch.cuda module. This module provides a simple way to interact with the GPU and perform various operations. The following code demonstrates how to check for GPU availability and display GPU information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "No GPU available, using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)  # Print the device being used\n",
    "print()\n",
    "\n",
    "# Additional Info when using CUDA (GPU)\n",
    "if device.type == 'cuda':  # If a GPU is being used\n",
    "    print(torch.cuda.get_device_name(0))  # Print the GPU name\n",
    "\n",
    "    print(\"Memory Usage:\")  # Print memory usage information\n",
    "\n",
    "    # Calculate and print the allocated GPU memory in GB\n",
    "    print(\"Allocated:\", round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    \n",
    "    # Calculate and print the cached (reserved) GPU memory in GB\n",
    "    print(\"Cached:\", round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "else:\n",
    "    # If no GPU is available, inform the user that the CPU is being used\n",
    "    print(\"No GPU available, using CPU instead.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, 1024**3 is used to convert the memory usage values from bytes to gigabytes.\n",
    "\n",
    "To move data from CPU memory to GPU memory, you can use the `to()` method or the `cuda()` method of a tensor. It is crucial to move both your model and the data to the GPU to fully benefit from GPU acceleration. The following code demonstrates how to move a tensor to the GPU.\n",
    "\n",
    "You can check if a tensor is on the GPU by using the `is_cuda` attribute of the tensor. The is_cuda attribute returns a boolean value, indicating whether the tensor is stored in GPU memory or not. Here's an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is tensor on GPU (CPU tensor)? False\n",
      "Is tensor on GPU (GPU tensor)? False\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor in CPU memory\n",
    "tensor_cpu = torch.zeros((10, 10))\n",
    "\n",
    "# Check if tensor is on GPU\n",
    "print(\"Is tensor on GPU (CPU tensor)?\", tensor_cpu.is_cuda)\n",
    "\n",
    "# Move the tensor to GPU memory\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tensor_gpu = tensor_cpu.to(device)\n",
    "\n",
    "# Check if tensor is on GPU (after moving it to GPU memory)\n",
    "print(\"Is tensor on GPU (GPU tensor)?\", tensor_gpu.is_cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that you should move both your model and the data to the GPU before performing any operations on them.\n",
    "\n",
    "By understanding how to utilize GPUs in PyTorch, especially in the context of Google Colab, you can accelerate your deep learning projects and optimize your models' performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Optimization and Gradient Descent\n",
    "\n",
    "Now, we delve into the core concept that underlies stochastic gradient descent (SGD), a widely employed optimization technique for training large-scale machine learning models. Our exploration begins with an overview of Gradient Descent, which lays the foundation for understanding the advantages of SGD in managing the training of massive architectures. \n",
    "\n",
    "Subsequently, we shift our attention to the autodifferentiation and backpropagation algorithm, an essential mechanism that empowers neural networks to learn effectively.\n",
    "\n",
    "Let us start with a very simple example of trying to find the minimum of the function\n",
    "$f: \\mathbb{R} \\rightarrow \\mathbb{R}$, where:\n",
    "$$\n",
    "f(x) = x^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 450x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def f(x):  # Objective function\n",
    "    return x ** 2\n",
    "\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "\n",
    "plt.plot(x, f(x))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, finding the local minima of more complicated functions, especially in the multivariate case, can be quite challenging. Fortunately, numerical methods such as the classical and powerful gradient descent come to our aid, forming the foundation for highly effective optimization algorithms that help manage the training of large-scale deep learning models.\n",
    "\n",
    "In essence, gradient descent enables us to find the minimum of a function by descending the slope and making meaningful moves in the direction of steepest descent, which is given by the negative of the gradient, until this process converges. Intuitively, when you no longer make significant progress with each new step, within a certain sensitivity, you have reached the minimum.\n",
    "\n",
    "Gradient descent consists of three main components:\n",
    "\n",
    "- Initializing a value for our parameters.\n",
    "\n",
    "- Updating our parameters in the opposite direction of the gradient with the learning rate determining the size of the jump. \n",
    "\n",
    "- Continuing to jump for a pre-defined number of steps, as gradient descent is guaranteed to converge to the global minimum for convex surfaces and to a local minimum for non-convex surfaces.\n",
    "\n",
    "Formally, we can express gradient descent (GD) as:\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - \\eta_{k} \\nabla f(\\mathbf{x}),\n",
    "$$\n",
    "\n",
    "Here, $\\eta$ represents the learning rate, which can be fixed or dynamic (i.e., changing at each step), denoted by $\\eta_{k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement the 1D case for our simple yet elegant function, $f(x) = x^2$. In this case, we know analytically that the gradient $f'(x) = 2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch k = 0, x: 6.000000\n",
      "epoch k = 10, x: 0.036280\n",
      "epoch k = 20, x: 0.000219\n",
      "epoch k = 30, x: 0.000001\n",
      "epoch k = 40, x: 0.000000\n",
      "epoch k = 50, x: 0.000000\n",
      "epoch k = 60, x: 0.000000\n",
      "epoch k = 70, x: 0.000000\n",
      "epoch k = 80, x: 0.000000\n",
      "epoch k = 90, x: 0.000000\n"
     ]
    }
   ],
   "source": [
    "def f_grad(x):  \n",
    "    # Gradient (derivative) of the objective function\n",
    "    return 2 * x \n",
    "\n",
    "def lr(k):\n",
    "    # Function that returns the learning rate at iteration k\n",
    "    return 0.2\n",
    "\n",
    "def gd(f_grad):\n",
    "    # Gradient descent function\n",
    "    max_iter = 100\n",
    "    x = 10.0\n",
    "    results = [x]\n",
    "    \n",
    "    for k in range(0, max_iter):\n",
    "        # Compute gradient at current point\n",
    "        g = f_grad(x) \n",
    "        \n",
    "        # Compute learning rate at current iteration\n",
    "        eta = lr(k)\n",
    "        \n",
    "        # Perform update using gradient and learning rate\n",
    "        x = x - eta * g \n",
    "        \n",
    "        # Store updated point\n",
    "        results.append(float(x))\n",
    "        \n",
    "        # Print iteration information\n",
    "        if k % 10 == 0:\n",
    "            print(f'epoch k = {k:.0f}, x: {x:f}')\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Call the gradient descent function with the objective function's gradient\n",
    "results = gd(f_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning tasks are reminiscent to the simple example above, where we aim to minimize a cost function (or a loss function) that represents the difference between the predicted output $f(x_i; \\theta)$ and the actual output $y_i$, where $x_i$ are the input data (e.g., images) and $\\theta$ are the model parameters we aim to optimize. The loss function $L(\\theta)$ is defined as:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\dfrac{1}{n} \\sum_{i=1}^n \\ell(y_i, f(x_i; \\theta))\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training samples, $y_i$ is the true output for input $x_i$, and $f(x_i; \\theta)$ is the predicted output for input $x_i$ using the current model parameters $\\theta$. The goal of the optimization algorithm is to find the optimal set of parameters $\\theta$ that minimizes the loss function $L(\\theta)$. Since we have multiple data entries, the total \"best\" gradient would be the average of the gradients over all the data entries.\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "\n",
    "However, if the number of training samples $n$ is very large (e.g., a billion data points), computing the gradient of the loss function for all training samples at once can be computationally expensive. This is where stochastic gradient descent (SGD) comes in.\n",
    "\n",
    "The main idea of SGD is to use a subset of the training samples to compute an approximation of the gradient of the loss function at each iteration. Specifically, at each iteration $k$, we randomly select a small subset of training samples (usually referred to as a \"mini-batch\") and use these samples to compute an approximation of the gradient of the loss function. The mini-batch gradient is then used to update the model parameters as follows:\n",
    "\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k - \\eta_k \\nabla L(\\theta_k; x_{i:i+b-1}, y_{i:i+b-1})\n",
    "$$\n",
    "\n",
    "where $b$ is the mini-batch size, $\\eta_k$ is the learning rate at iteration $k$, and $\\nabla L(\\theta_k; x_{i:i+b-1}, y_{i:i+b-1})$ is the gradient of the loss function with respect to the model parameters $\\theta_k$ computed using the mini-batch of training samples $(x_{i:i+b-1}, y_{i:i+b-1})$.\n",
    "\n",
    "The main advantage of SGD over gradient descent (GD) is that it requires less memory and computational resources, making it more suitable for large-scale machine learning problems. However, SGD also has some disadvantages. Since the mini-batches are selected randomly, the stochasticity of the algorithm can result in high variance and slow convergence of the optimization process. Also, the learning rate $\\eta_k$ needs to be carefully tuned to ensure convergence of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodifferentiation\n",
    "\n",
    "While optimizing a machine learning model (which we will see more advanced examples of in notebook 2-4) might seem conceptually simple, i.e., we can simply perform gradient descent by differentiating the loss function $L(\\theta)$ to find the best model that fits the data, the main challenge here is how we can calculate the gradient. In the simple example $f(x) = x^2$ above, this is straightforward, because we know the analytic closed form of the differentiation. However, for a very complex neural network (i.e., a very complicated function), analytically writing down the analytic gradient can be very difficult. And this is where modern machine learning frameworks such as PyTorch, TensorFlow, and JAX come to the rescue.\n",
    "\n",
    "The central component of any machine learning framework is what we call autodifferentiation, which is performed through the backpropagation algorithm. At its core, backpropagation is based on the chain rule of differentiation, which allows us to compute the derivative of a function composed of simpler functions. This makes differentiation a simpler process compared to integration, which is why backpropagation is an effective method for optimizing deep neural networks.\n",
    "\n",
    "But before we dive deeper into backpropagation, let's discuss the different methods of differentiation.\n",
    "\n",
    "There are three common methods to perform differentiation: **finite differences**, **symbolic differentiation**, and **autodifferentiation**.\n",
    "\n",
    "Finite difference is a numerical method of approximating the derivative of a function by calculating the difference between function values at two nearby points. Although simple to implement, it can be computationally expensive and imprecise, especially for high-dimensional functions. The computational complexity of this method is proportional to the number of dimensions squared.\n",
    "\n",
    "Symbolic differentiation, on the other hand, involves deriving the mathematical formula for the derivative of a function using algebraic manipulation of the function's equation, such as the one implemented in Mathematica. This method is precise and exact, but it can be complex and time-consuming, especially for functions with many variables or complex expressions. Symbolic differentiation has a fixed computational complexity that depends on the complexity of the function being differentiated.\n",
    "\n",
    "In contrast to finite differences and symbolic differentiation, **autodifferentiation** is a powerful technique that enables us to compute gradients automatically, without having to manually derive and implement the gradients. In other words, we can focus on specifying the structure of the neural network and defining the loss function, and the framework takes care of the differentiation for us. This greatly simplifies the process of designing and training deep neural networks, and allows us to experiment with more complex architectures and loss functions.\n",
    "\n",
    "Another advantage of autodifferentiation is that it allows us to compute gradients efficiently, which is crucial for large-scale machine learning problems. In addition, many frameworks provide support for distributed training, which allows us to train deep neural networks on clusters of GPUs or TPUs, further accelerating the training process.\n",
    "\n",
    "One downside of autodifferentiation is that it is completely numerical, and hence it would not yield any analytic closed form. However, for most cases in machine learning, where the goal is to numerically update the model parameters, this limitation is not critical.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation and Chain Rule\n",
    "\n",
    "To understand how backpropagation works, let's consider a slightly more nontrivial example where input vector $x$ and produces an output $h$ using a linear transformation followed by a sigmoid activation function:\n",
    "\n",
    "$$\n",
    "z = Wx + b,\n",
    "$$\n",
    "\n",
    "$$\n",
    "h = \\sigma(z) = \\dfrac{1}{1+e^{-z}},\n",
    "$$\n",
    "\n",
    "where $W$ is a weight matrix, $b$ is a bias vector, and $\\sigma$ is the sigmoid function.\n",
    "\n",
    "Our goal is to optimize the parameters $W$ and $b$ to best fit the data. Given a single data point $(x,y)$, we use the mean squared error (MSE) loss function:\n",
    "\n",
    "$$\n",
    "\\ell(W,b|x,y) = (y - \\sigma(Wx + b))^2.\n",
    "$$\n",
    "\n",
    "Our aim is to calculate the gradients of the loss function with respect to the parameters $W$ and $b$, which will allow us to update the parameters using gradient descent.\n",
    "\n",
    "The chain rule states that if we have a function $\\ell$ that is composed of several simpler functions then the derivative of $\\ell$ with respect to $W$ is given by the product of the derivatives of the simpler functions with respect to $W$. In other words,\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial \\ell}{\\partial W} = \\dfrac{\\partial \\ell}{\\partial h} \\dfrac{\\partial h}{\\partial z} \\dfrac{\\partial z}{\\partial W},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial \\ell}{\\partial b} = \\dfrac{\\partial \\ell}{\\partial h} \\dfrac{\\partial h}{\\partial z} \\dfrac{\\partial z}{\\partial b},\n",
    "$$\n",
    "\n",
    "where $\\dfrac{\\partial h}{\\partial z}$ is the differentiation of the sigmoid function\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial h}{\\partial z}  = \\dfrac{\\partial \\sigma}{\\partial z} = \\sigma(z)(1-\\sigma(z)),\n",
    "$$\n",
    "\n",
    "and $\\dfrac{\\partial z}{\\partial W} = x$, $\\dfrac{\\partial z}{\\partial b} = 1$, and\n",
    "\n",
    "$$\n",
    "\\dfrac{\\partial \\ell}{\\partial h} = 2 (y - h).\n",
    "$$\n",
    "\n",
    "While all these calculations might seem straightforward, what PyTorch does implicitly is that it calculates the loss for all input pairs $\\{x_i,y_i\\}$ in batch, and therefore is able to calculate the gradients in parallel and take the mean of a batch of inputs, which is critical for stochastic gradient descent. \n",
    "\n",
    "### Forward and Backward Passing\n",
    "\n",
    "It is worth noting that the term $\\dfrac{\\partial \\ell}{\\partial h}$ can be shared for the calculation of $\\dfrac{\\partial \\ell}{\\partial W}$ and $\\dfrac{\\partial \\ell}{\\partial b}$. Therefore, if we are smart algorithmically, we need only calculate this term once and reuse it. This leads to the first key concept of autodiff, which is known as dynamic programming, i.e., we algorithmically collect all the terms that is needed to calculate both the outputs and all the intermediate differentiations. And this is done with a \"forward pass\" to calculate the output, and then a \"backward pass\" to collect all the intermediate differentiations.\n",
    "\n",
    "Algorithmically, for the forward passing: \n",
    "\n",
    "- Initialize the weights and biases $W$ and $b$.\n",
    "\n",
    "- Pass the input $x$ through the linear transformation $Wx + b$ to obtain $z = W x + b$.\n",
    "\n",
    "- Apply the sigmoid activation function to $z$ to obtain the output $h = \\sigma(z)$.\n",
    "\n",
    "- Calculate the loss $\\ell$.\n",
    "\n",
    "Now, let's move on to the backward pass:\n",
    "\n",
    "- Calculate the derivative of the loss function $\\ell$ with respect to the output $h$: $\\dfrac{\\partial \\ell}{\\partial h} = 2 (y - h)$.\n",
    "\n",
    "- Calculate the derivative of $h$ with respect to $z$: $\\dfrac{\\partial h}{\\partial z} = \\sigma(z)(1-\\sigma(z))$.\n",
    "\n",
    "- Calculate the derivative of the loss function with respect to $z$: $\\dfrac{\\partial \\ell}{\\partial z} = \\dfrac{\\partial \\ell}{\\partial h} \\dfrac{\\partial h}{\\partial z}$.\n",
    "\n",
    "- Calculate the derivative of $z$ with respect to $W$: $\\dfrac{\\partial z}{\\partial W} = x$.\n",
    "\n",
    "- Calculate the derivative of $z$ with respect to $b$: $\\dfrac{\\partial z}{\\partial b} = 1$.\n",
    "\n",
    "- Calculate the derivative of the loss function with respect to $W$: $\\dfrac{\\partial \\ell}{\\partial W} = \\dfrac{\\partial \\ell}{\\partial z} \\dfrac{\\partial z}{\\partial W}$.\n",
    "\n",
    "- Calculate the derivative of the loss function with respect to $b$: $\\dfrac{\\partial \\ell}{\\partial b} = \\dfrac{\\partial \\ell}{\\partial z} \\dfrac{\\partial z}{\\partial b}$.\n",
    "\n",
    "During the forward pass, PyTorch builds a computation graph by tracking all the operations performed on the input data. Each node in the graph represents a tensor, and each edge represents an operation that transforms one tensor into another. The computation graph is then used during the backward pass to compute gradients of the loss function with respect to the input tensors.\n",
    "\n",
    "During the backward pass, PyTorch first sets the gradient of the loss function with respect to the final output tensor to 1. Then, starting from the final output tensor, PyTorch uses the computation graph to compute the gradients of the loss function with respect to all the input tensors. This is done by applying the chain rule of differentiation to each operation in the computation graph.\n",
    "\n",
    "For example, consider the computation graph for the simple example of backpropagation we have been discussing. The computation graph would look be:\n",
    "\n",
    "$$\n",
    "x \\overset{(W,b)}{\\longrightarrow} z \\rightarrow h \\rightarrow \\ell\n",
    "$$\n",
    "\n",
    "where each arrow represents an operation, and the direction of the arrow indicates the flow of data.\n",
    "\n",
    "And for the backward pass, propagate from the end point back to the original leaf to avoid any redundant calculation of intermediate differentiations and ensure that all the input needed in the intermediate steps have been calculated.\n",
    "$$\n",
    "1 \\rightarrow \\dfrac{\\partial \\ell}{\\partial h} \\rightarrow \\dfrac{\\partial \\ell}{\\partial z} \\rightarrow \\bigg(\\dfrac{\\partial \\ell}{\\partial W}, \\dfrac{\\partial \\ell}{\\partial b} \\bigg)\n",
    "$$\n",
    "\n",
    "Once we have calculated the gradients, we can update the parameters using gradient descent, taking the mean gradient of the individual batches.\n",
    "\n",
    "### Other Advantages of Backpropagation\n",
    "\n",
    "In addition to its memory-efficient dynamic programming capabilities, collecting gradients through backpropagation has two more advantages. Firstly, backpropagation works for any parameter, even if they are in the middle of the computational graph. In the example discussed earlier, the parameters $W$ and $b$ are leaf nodes, which are at the entry point of the entire computational graph. However, in more complex neural networks, parameters can be located anywhere in the graph, and backpropagation can still be used to compute their gradients.\n",
    "\n",
    "Secondly, the gradient Jacobian matrix tends to have a lower dimension at the output layer, where the loss function is a scalar value. This makes the backpropagation algorithm more efficient as it avoids massive matrix multiplication. However, the dimensions of the individual Jacobian matrices and the computational complexity of backpropagation can vary depending on the complexity of the neural network being used.\n",
    "\n",
    "It is important to note that backpropagation is not limited to the simple example discussed earlier. It can be applied to neural networks with any number of layers and any activation functions. The only difference is that the computation graph becomes more complex, with more nodes and edges, and the chain rule of differentiation needs to be applied recursively to each node in the graph. With the help of modern machine learning frameworks such as PyTorch, TensorFlow, and JAX, backpropagation has become a powerful tool for optimizing deep neural networks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation in Pytorch\n",
    "\n",
    "Backpropagation is a powerful technique that allows us to compute gradients of complex functions automatically, without having to manually derive and implement the gradients. As long as the operators in the function are all analytic and differentiable, we can in principle calculate the gradients of arbitrary complex functions using backpropagation.\n",
    "\n",
    "To demonstrate how backpropagation works in practice, let's revisit the simple example of $f(x) = x^2$. We want to compute the gradient of this function with respect to the input x, which is simply $f'(x) = 2x$. We can implement this function in PyTorch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    # Define function\n",
    "    return x**2\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True) # Define input variable\n",
    "y = f(x) # Compute function value\n",
    "y.backward() # Compute gradients with respect to x\n",
    "\n",
    "print(x.grad) # Output gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code, the input variable $x$ is defined as a PyTorch tensor with `requires_grad=True`. The `requires_grad` attribute of the tensor indicates that we want to compute gradients with respect to $x$. The function $f(x)$ is defined as $x^2$, and the value of the function at $x$ is computed using $y = f(x)$.\n",
    "\n",
    "The `backward()` method is then called on $y$ to compute the gradient of $y$ with respect to $x$. Backpropagation is automatically performed by PyTorch, and the gradient is computed using the chain rule of differentiation. Although for this simple function, the chain rule is not needed.\n",
    "\n",
    "Once we have the gradient, we can use it to find the minimal point of the function. This is typically done using an optimization algorithm such as gradient descent. In gradient descent, the gradient is used to update the value of $x$ in the direction that minimizes the function. The `lr` variable in the previous code snippet corresponds to the learning rate of the gradient descent algorithm, which determines the step size of each update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch k = 0, x: 6.000000\n",
      "epoch k = 10, x: 0.036280\n",
      "epoch k = 20, x: 0.000219\n",
      "epoch k = 30, x: 0.000001\n",
      "epoch k = 40, x: 0.000000\n",
      "epoch k = 50, x: 0.000000\n",
      "epoch k = 60, x: 0.000000\n",
      "epoch k = 70, x: 0.000000\n",
      "epoch k = 80, x: 0.000000\n",
      "epoch k = 90, x: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Objective function we want to minimize\n",
    "def objective_function(x):\n",
    "    return x**2\n",
    "\n",
    "# Gradient descent function using PyTorch\n",
    "def gd_pytorch(objective_function, lr=0.2, max_iter=100):\n",
    "    \n",
    "    # Initialize x\n",
    "    x = torch.tensor([10.0], requires_grad=True)\n",
    "    results = [x.item()]\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        \n",
    "        # Compute objective function at current point\n",
    "        f = objective_function(x)\n",
    "        \n",
    "        # Compute gradient of objective function with respect to x\n",
    "        f.backward()\n",
    "        \n",
    "        # Perform update using gradient and learning rate\n",
    "        with torch.no_grad():\n",
    "            x -= lr * x.grad\n",
    "        \n",
    "        # Clear gradient for next iteration\n",
    "        x.grad.zero_()\n",
    "        \n",
    "        # Store updated point\n",
    "        results.append(x.item())\n",
    "        \n",
    "        # Print iteration information\n",
    "        if k % 10 == 0:\n",
    "            print(f'epoch k = {k:.0f}, x: {x.item():f}')\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Call the gradient descent function with the objective function\n",
    "results = gd_pytorch(objective_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We indeed find the minimal point at $x=0$.\n",
    "\n",
    "In the loop, compute the objective function at the current point, then use `backward()` method to compute the gradient of the objective function with respect to $x$. Perform the update using the computed gradient and learning rate inside the `torch.no_grad()` context manager to avoid building a computation graph during the update. Clear the gradient for the next iteration using `x.grad.zero_()`.\n",
    "\n",
    "In PyTorch, when we perform an operation on a tensor, PyTorch keeps track of the computation graph that was used to compute the output tensor. This graph is then used during the backward pass to compute the gradients of the output tensor with respect to the input tensors.\n",
    "\n",
    "However, sometimes we want to perform operations that should not be tracked by the computation graph, such as when we are updating the parameters of a model during training. This is where the `torch.no_grad()` context manager comes in handy.\n",
    "\n",
    "When we wrap a block of code with `torch.no_grad()`, any operations that are performed within that block are not tracked by the computation graph. This is useful when we want to compute the output of a model without updating its parameters. In the context of the gradient descent algorithm, we want to update the value of the input tensor without building a computation graph for the update operation, so we wrap the update operation with `torch.no_grad()`.\n",
    "\n",
    "Similarly, in the line `x.grad.zero_()`, we are zeroing out the gradients of the input tensor $x$. This is necessary because PyTorch accumulates gradients by default, meaning that if we call `backward()` multiple times on the same tensor, the gradients will be accumulated. This is useful in some cases, but in our case, we want to compute the gradient of the objective function with respect to the input tensor for each iteration of the gradient descent algorithm, so we need to zero out the gradient at the beginning of each iteration using `x.grad.zero_()`. This ensures that the gradient computed in the previous iteration does not affect the gradient computed in the current iteration.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Slightly More Complicated Example\n",
    "\n",
    "Let's try to go beyond this simple example and let's fit the sigmoid function as we discussed above. First, we'll generate a mock dataset with `torch.randn` for the input $x$ and compute the corresponding output $y$ using the sigmoid function. We'll also add some noise to $y$ to make the problem more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 450x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate mock dataset\n",
    "n = 100\n",
    "x = torch.randn(n, 1)\n",
    "y = torch.sigmoid(3*x - 1 + 0.5*torch.randn(n, 1))\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the sigmoid function as a PyTorch module, which will allow us to easily optimize its parameters using autodifferentiation and backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll initialize the parameters of the sigmoid function W and b with random values using torch.randn. We'll also set `requires_grad=True` for both parameters so that we can compute gradients with respect to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters with random values\n",
    "W = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the objective function as the mean squared error between the predicted output $y_{pred}$ and the actual output $y$. We'll use the Sigmoid module to compute the predicted output $y_{pred}$ from the input $x$ and the parameters $W$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define objective function\n",
    "criterion = torch.nn.MSELoss()\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "def objective_fn(W, b):\n",
    "    y_pred = sigmoid(W*x + b)\n",
    "    loss = criterion(y_pred, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will optimize the parameters $W$ and $b$ using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.2327\n",
      "epoch 500, loss: 0.0101\n",
      "epoch 1000, loss: 0.0061\n",
      "epoch 1500, loss: 0.0051\n",
      "epoch 2000, loss: 0.0047\n",
      "epoch 2500, loss: 0.0045\n",
      "epoch 3000, loss: 0.0044\n",
      "epoch 3500, loss: 0.0043\n",
      "epoch 4000, loss: 0.0043\n",
      "epoch 4500, loss: 0.0043\n",
      "epoch 5000, loss: 0.0043\n",
      "epoch 5500, loss: 0.0043\n",
      "epoch 6000, loss: 0.0043\n",
      "epoch 6500, loss: 0.0043\n",
      "epoch 7000, loss: 0.0043\n",
      "epoch 7500, loss: 0.0043\n",
      "epoch 8000, loss: 0.0043\n",
      "epoch 8500, loss: 0.0043\n",
      "epoch 9000, loss: 0.0043\n",
      "epoch 9500, loss: 0.0043\n"
     ]
    }
   ],
   "source": [
    "max_iter = 10000\n",
    "lr = 0.1\n",
    "\n",
    "results = []\n",
    "for i in range(max_iter):\n",
    "    loss = objective_fn(W, b)\n",
    "    if i % 500 == 0:\n",
    "        print(f'epoch {i}, loss: {loss.item():.4f}')\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W -= lr * W.grad\n",
    "        b -= lr * b.grad\n",
    "        W.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "    results.append((W.item(), b.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the predicted output of the optimized sigmoid function along with the original data to see how well the model fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 450x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot predicted output of optimized sigmoid function\n",
    "W_opt, b_opt = results[-1]\n",
    "y_pred_opt = sigmoid(W_opt*x + b_opt)\n",
    "plt.scatter(x, y)\n",
    "sorted_indices = x.squeeze().sort()[1]\n",
    "plt.plot(x[sorted_indices], y_pred_opt[sorted_indices], color='red', lw=5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot should show the original data points along with a red line representing the predicted output of the optimized sigmoid function. The goal of the optimization is to find the values of $W$ and $b$ that minimize the mean squared error between the predicted output and the actual output. By visual inspection, we can see that the optimized sigmoid function fits the data quite well.\n",
    "\n",
    "Finally, instead of writing our own gradient descend codes, we can also employ the PyTorch optimization `torch.optim` module, which will make this example much more concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 0.0261\n",
      "Epoch 200, Loss: 0.0081\n",
      "Epoch 300, Loss: 0.0042\n",
      "Epoch 400, Loss: 0.0027\n",
      "Epoch 500, Loss: 0.0018\n",
      "Epoch 600, Loss: 0.0013\n",
      "Epoch 700, Loss: 0.0010\n",
      "Epoch 800, Loss: 0.0007\n",
      "Epoch 900, Loss: 0.0005\n",
      "Epoch 1000, Loss: 0.0004\n",
      "Epoch 1100, Loss: 0.0003\n",
      "Epoch 1200, Loss: 0.0002\n",
      "Epoch 1300, Loss: 0.0002\n",
      "Epoch 1400, Loss: 0.0002\n",
      "Epoch 1500, Loss: 0.0001\n",
      "Epoch 1600, Loss: 0.0001\n",
      "Epoch 1700, Loss: 0.0001\n",
      "Epoch 1800, Loss: 0.0001\n",
      "Epoch 1900, Loss: 0.0000\n",
      "Epoch 2000, Loss: 0.0000\n",
      "Epoch 2100, Loss: 0.0000\n",
      "Epoch 2200, Loss: 0.0000\n",
      "Epoch 2300, Loss: 0.0000\n",
      "Epoch 2400, Loss: 0.0000\n",
      "Epoch 2500, Loss: 0.0000\n",
      "Epoch 2600, Loss: 0.0000\n",
      "Epoch 2700, Loss: 0.0000\n",
      "Epoch 2800, Loss: 0.0000\n",
      "Epoch 2900, Loss: 0.0000\n",
      "Epoch 3000, Loss: 0.0000\n",
      "Epoch 3100, Loss: 0.0000\n",
      "Epoch 3200, Loss: 0.0000\n",
      "Epoch 3300, Loss: 0.0000\n",
      "Epoch 3400, Loss: 0.0000\n",
      "Epoch 3500, Loss: 0.0000\n",
      "Epoch 3600, Loss: 0.0000\n",
      "Epoch 3700, Loss: 0.0000\n",
      "Epoch 3800, Loss: 0.0000\n",
      "Epoch 3900, Loss: 0.0000\n",
      "Epoch 4000, Loss: 0.0000\n",
      "Epoch 4100, Loss: 0.0000\n",
      "Epoch 4200, Loss: 0.0000\n",
      "Epoch 4300, Loss: 0.0000\n",
      "Epoch 4400, Loss: 0.0000\n",
      "Epoch 4500, Loss: 0.0000\n",
      "Epoch 4600, Loss: 0.0000\n",
      "Epoch 4700, Loss: 0.0000\n",
      "Epoch 4800, Loss: 0.0000\n",
      "Epoch 4900, Loss: 0.0000\n",
      "Epoch 5000, Loss: 0.0000\n",
      "Optimized parameters: W = 1.9963, b = -0.9984\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Generate mock dataset\n",
    "x = torch.randn(100, 1)\n",
    "y = torch.sigmoid(2*x - 1)\n",
    "\n",
    "#------------------------------------------------\n",
    "# define sigmoid model\n",
    "class SigmoidModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "model = SigmoidModel()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5000):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "#------------------------------------------------\n",
    "# retrieve optimized parameters\n",
    "w_opt, b_opt = model.linear.weight.item(), model.linear.bias.item()\n",
    "\n",
    "# print optimized parameters\n",
    "print(f'Optimized parameters: W = {w_opt:.4f}, b = {b_opt:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explore the `grad_fn` attribute of a PyTorch tensor, which provides information about the operation that was performed to produce the tensor, as well as its inputs. We can use the `next_functions` attribute to explore the graph further by following the edges backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward0 object at 0x7fede1299670>\n",
      "<SigmoidBackward0 object at 0x7fede1299a30>\n",
      "<AddmmBackward0 object at 0x7fede1299670>\n",
      "<AccumulateGrad object at 0x7fede1299910>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # should print \"MseLossBackward()\"\n",
    "print(loss.grad_fn.next_functions[0][0])  # should print \"SigmoidBackward()\"\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # should print \"AddmmBackward()\"\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0])  # should print \"AccumulateGrad()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these objects represents the operation that was performed on the tensor to produce the current node in the computation graph.\n",
    "\n",
    "The first object, `MseLossBackward0`, represents the backward pass of the mean squared error loss. This object computes the gradient of the loss with respect to its inputs, which in this case is the output of the sigmoid function.\n",
    "\n",
    "The second object, `SigmoidBackward0`, represents the backward pass of the sigmoid function. This object computes the gradient of the sigmoid function with respect to its inputs, which in this case is the weighted sum of the input features.\n",
    "\n",
    "The third object, `AddmmBackward0`, represents the backward pass of the linear transformation ($x \\rightarrow Wx + b$). This object computes the gradient of the linear transformation with respect to its inputs, which in this case is the input features.\n",
    "\n",
    "The fourth object, `AccumulateGrad`, represents the accumulation of gradients during the backward pass. This object is used to accumulate gradients from multiple backward passes, which is necessary when performing gradient descent over an ensemble of data points.\n",
    "\n",
    "By examining the `grad_fn` attributes of the tensors in the computation graph, we can trace back the operations that were performed on the inputs to produce the output, and we can use this information to compute the gradients of the parameters with respect to the loss function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We've learned in this tutorial:\n",
    "\n",
    "- **Pytorch basics**: We learned the basics of Pytorch and its connection with NumPy. Pytorch is a Python-based scientific computing package that allows us to perform numerical operations using GPUs, which can significantly speed up computation. Pytorch is built on top of the Torch library and offers a wide range of tools for building and training machine learning models.\n",
    "\n",
    "- **GPU acceleration**: We learned how to accelerate our computations using GPUs in Pytorch. Pytorch has a CUDA backend that allows us to utilize GPUs to speed up computations, especially for large-scale machine learning problems.\n",
    "\n",
    "- **Gradient descent**: We learned about gradient descent, which is an optimization algorithm used to minimize a cost function by iteratively adjusting the parameters of a machine learning model. We discussed stochastic gradient descent and how they are used in practice.\n",
    "\n",
    "- **Autodifferentiation and backpropagation**: We learned about autodifferentiation and backpropagation, which are key techniques for computing gradients in Pytorch. Autodifferentiation allows us to compute gradients automatically without having to derive them manually, while backpropagation allows us to efficiently propagate gradients backward through a computational graph.\n",
    "\n",
    "- **Pytorch implementation**: We learned how to implement gradient descent, autodifferentiation, and backpropagation in Pytorch using various functions and modules. We also learned how to use Pytorch's built-in optimization functions and tools, such as the torch.optim module.\n",
    "\n",
    "Overall, this tutorial provided a comprehensive introduction to Pytorch and its applications in machine learning. Armed with this knowledge, we can now start building more complex models and experimenting with different neural network architectures in the following tutorials.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : Implementy Autodifferentiation and Backpropagation Using Only Numpy\n",
    "\n",
    "(credit: Part of the codes here are adapted from a tutorial in [COMP4670/8600](https://www.sml2023.com/) that I taught, prepared by tutor Chamin Hewa Koneputugodage)\n",
    "\n",
    "We've demonstrated that PyTorch can make complex neural networks modular and easy to optimize through backpropagation. Central to any autodifferentiation package like PyTorch is the ability to store not only the output value of a function in a computational graph but also the derivatives of that function with respect to the input variables. This is why, in PyTorch, when we set `required_grad=True`, the size of the array doubles to store not just the output but also the local gradients. During backpropagation, these local gradients are used to iteratively calculate all the gradients within the computational graph.\n",
    "\n",
    "While the concept may seem straightforward, the implementation details can be quite intricate. However, it is possible to build backpropagation functionality and neural networks using only Numpy, albeit with less optimization compared to PyTorch. We'll use the Sigmoid example from before and build individual layers for the computational nodes, including the automatic storage of gradients.\n",
    "\n",
    "By understanding the details below, you'll gain a deeper understanding of backpropagation.\n",
    "\n",
    "First, let's implement the fully connected layer, which performs the operation $\\mathbf{y} = \\mathbf{W} \\cdot \\mathbf{x} + \\mathbf{b}$. In PyTorch, this layer is referred to as a Linear layer. The fully connected layer is a fundamental component of most neural networks, as it connects the input and output neurons within and between layers. It is responsible for transforming the input data using weights $\\mathbf{w}$ and biases $\\mathbf{b}$ to produce the output $\\mathbf{y}$.\n",
    "\n",
    "The weights and biases are learnable parameters that are adjusted during the training process to minimize the loss function. In a fully connected layer, each output neuron is connected to every input neuron, allowing the layer to learn complex patterns and relationships in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructor for the FullyConnectedLayer class\n",
    "class FullyConnectedLayer():\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.in_features = in_features  # Number of input features\n",
    "        self.out_features = out_features  # Number of output features\n",
    "        self.weight = np.zeros((in_features, out_features))  # Weight matrix: W\n",
    "        self.bias = np.zeros((out_features, 1))  # Bias vector: b\n",
    "        self.g_weight = np.zeros((in_features, out_features))  # Gradient of weight matrix: dy/dW\n",
    "        self.g_bias = np.zeros((out_features, 1))  # Gradient of bias vector: dy/db\n",
    "        self.input = None  # Placeholder for input data\n",
    "\n",
    "    # Initialize the weights and biases with random values\n",
    "    def init_weights(self):        \n",
    "        self.weight = np.random.randn(self.in_features, self.out_features)\n",
    "        self.bias = np.random.rand(self.out_features, 1)\n",
    "\n",
    "    # Perform forward pass through the layer\n",
    "    def forward_pass(self, X):        \n",
    "        self.input = X  # Store input data: x\n",
    "        out = np.dot(X, self.weight) + self.bias.T  # Compute the output: y(x|W,b)\n",
    "        return out\n",
    "        \n",
    "    # Perform backward pass through the layer\n",
    "    # Note that g_next_layer is the gradient from the future layer\n",
    "    # recall from the lecture that the gradient for the current layer take into account the gradient from the future layer\n",
    "    # by the chain rule (matrix multiplication)\n",
    "    def backward_pass(self, g_next_layer):\n",
    "        self.g_weight = np.dot(self.input.T, g_next_layer)  # Compute gradient of weight matrix\n",
    "        self.g_bias = np.sum(g_next_layer, axis=0, keepdims=True)  # Compute gradient of bias vector\n",
    "        g_last_layer = np.dot(g_next_layer, self.weight.T)  # Compute gradient of the input data for the current layer: dy/dx\n",
    "        return g_last_layer # we return the graident of the input data for the current layer so that it can be used in the previous layer\n",
    "\n",
    "    # Update weights and biases using the computed gradients\n",
    "    def update(self, learning_rate):\n",
    "        self.weight -= learning_rate*self.g_weight  # Update weight matrix\n",
    "        self.bias -= learning_rate*self.g_bias.T  # Update bias vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement sigmoid function and sigmoid layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "# Constructor for the Sigmoid class\n",
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.input = None  # Placeholder for input data\n",
    "        self.ctx = None  # Placeholder for output of the sigmoid function\n",
    "\n",
    "    # Perform forward pass through the sigmoid activation function\n",
    "    def forward_pass(self, X):\n",
    "        self.input = X  # Store input data\n",
    "        self.ctx = sigmoid(X)  # Apply sigmoid function and store the result\n",
    "        return self.ctx\n",
    "\n",
    "    # Perform backward pass through the sigmoid activation function\n",
    "    def backward_pass(self, g_next_layer):\n",
    "\n",
    "        # Compute the derivative of the sigmoid function, while taking into account the gradient from the future layer\n",
    "        # recall that the gradient of the sigmoid function is: dsigma/dx = sigma(x) * (1 - sigma(x))\n",
    "        g_last_layer = self.ctx * (1 - self.ctx) * g_next_layer\n",
    "        return g_last_layer\n",
    "\n",
    "    # No update needed for the Sigmoid activation function,\n",
    "    # since it does not have any parameters to update\n",
    "    def update(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructor for the MeanSquaredErrorLoss class\n",
    "class MSELoss():\n",
    "    def __init__(self):\n",
    "        self.input_y = None  # Placeholder for input predictions\n",
    "        self.input_t = None  # Placeholder for input targets: y_true\n",
    "        self.input_N = None  # Placeholder for batch size \n",
    "\n",
    "    # Perform forward pass through the MSE loss function\n",
    "    def forward_pass(self, y, t):\n",
    "        if len(t.shape) == 1:\n",
    "            t = t[:, None]  # Reshape targets if necessary\n",
    "\n",
    "        self.input_y = y  # Store input predictions\n",
    "        self.input_t = t  # Store input targets\n",
    "        self.input_N = y.shape[0]  # Store batch size\n",
    "\n",
    "        loss = (y - t)**2  # Calculate squared error for each prediction and target\n",
    "\n",
    "        return np.mean(loss)  # Return the mean of the loss\n",
    "    \n",
    "    # Perform backward pass through the MSE loss function\n",
    "    def backward_pass(self, g_next_layer = 1):\n",
    "\n",
    "        # Compute gradient of the MSE loss function, while taking into account the gradient from the future layer\n",
    "        # recall that the gradient of the MSE loss function is: dMSE/dy = 2 * (y - t)\n",
    "        # the 1/N factor is because we are using the mean of the loss \n",
    "        # and when we sum up the gradients in stochastic gradient descent, we need to divide by the batch size\n",
    "        g_last_layer = 2 * (self.input_y - self.input_t) / self.input_N * g_next_layer\n",
    "        return g_last_layer\n",
    "\n",
    "    # No update needed for the MSE loss function, so this method is a placeholder\n",
    "    def update(self, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the essential building blocks, we can combine them to create a simple (one-layer) yet functional neural network. This network will be based on the Sigmoid example we discussed earlier. The neural network will consist of a fully connected layer followed by a Sigmoid activation function. This basic architecture is useful for understanding how the different components interact and how data flows through the network.\n",
    "\n",
    "To construct this neural network, we will:\n",
    "\n",
    "- Create a FullyConnectedLayer object with a specified number of input and output features. This layer is responsible for performing the linear transformation on the input data.\n",
    "\n",
    "- Initialize the weights and biases for the FullyConnectedLayer using the init_weights() method. This ensures that our network starts with random weights and biases, which is essential for the training process.\n",
    "\n",
    "- Create a Sigmoid object that will apply the sigmoid activation function to the output of the fully connected layer. This activation function is responsible for introducing nonlinearity into the network, allowing it to learn complex patterns in the input data.\n",
    "\n",
    "- Implement a `forward_pass()` method that takes input data and feeds it through the network. The method will call the `forward_pass()` functions of each layer and activation function in the correct order.\n",
    "\n",
    "- Implement a `backward_pass()` method that computes the gradients of the loss function with respect to the network's parameters. This method will call the `backward_pass()` functions of each layer and activation function in the reverse order, effectively applying the chain rule to compute gradients.\n",
    "\n",
    "- Implement an `update()` method that updates the weights and biases of the network using the computed gradients and a specified learning rate.\n",
    "\n",
    "By assembling these components, we create a simple neural network that can be trained using gradient descent or other optimization algorithms to learn patterns in the input data. This basic architecture can serve as a foundation for building more complex and sophisticated neural networks in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    # Constructor for the Network class\n",
    "    def __init__(self):\n",
    "        self.sequential = []  # List to store the layers in the network\n",
    "        \n",
    "        # Create the first fully connected layer, we use 1 input feature and 1 output feature\n",
    "        fc1 = FullyConnectedLayer(1, 1)\n",
    "        fc1.init_weights()  # Initialize the weights and biases for the first fully connected layer\n",
    "        self.sequential.append(fc1)  # Add the first fully connected layer to the network\n",
    "\n",
    "        # Create the first sigmoid activation function\n",
    "        sigmoid1 = Sigmoid()\n",
    "        self.sequential.append(sigmoid1)  # Add the sigmoid activation function to the network\n",
    "        \n",
    "    # Perform forward pass through the network\n",
    "    def forward_pass(self, X):\n",
    "        for l in self.sequential:\n",
    "            X = l.forward_pass(X)  # Pass the input through each layer in the network\n",
    "        return X\n",
    "\n",
    "    # Perform backward pass through the network\n",
    "    def backward_pass(self, grad):\n",
    "        for l in reversed(self.sequential):\n",
    "            grad = l.backward_pass(grad)  # Pass the gradient through each layer in reverse order\n",
    "            \n",
    "    # Update the learnable parameters in the network\n",
    "    def update(self, learning_rate):\n",
    "        for l in self.sequential:\n",
    "            l.update(learning_rate)  # Update the learnable parameters in each layer using the given learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "net = Network()\n",
    "mse = MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now put our model to the test by generating a simple mock dataset that follows the Sigmoid function and attempt to recover the underlying function. This exercise will help us understand how well our neural network can learn from data and generalize the learned patterns to make accurate predictions.\n",
    "\n",
    "To accomplish this, we will:\n",
    "\n",
    "- Generate a synthetic dataset that consists of input-output pairs representing the Sigmoid function. The input values will be randomly sampled within a specific range, and the corresponding output values will be calculated using the Sigmoid function.\n",
    "\n",
    "- Split the generated dataset into training and testing sets. The training set will be used to train our model, while the testing set will be used to evaluate its performance on unseen data.\n",
    "\n",
    "- Train the model using the training set by iterating through the dataset for a specified number of epochs. In each epoch, we will perform a forward pass, calculate the loss, perform a backward pass to compute the gradients, and update the network's parameters using the computed gradients and a specified learning rate.\n",
    "\n",
    "- Evaluate the performance of the model on the testing set by calculating the mean squared error (MSE) loss between the predicted values and the ground truth values. This will give us an indication of how well the network has learned the underlying Sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_data(num_samples):\n",
    "    X = np.random.rand(num_samples, 1) * 10 - 5  # Generate random input data in range [-5, 5]\n",
    "    y = 1 / (1 + np.exp(-X))  # Apply the sigmoid function to the input data\n",
    "    return X, y\n",
    "\n",
    "num_samples = 1000  # Specify the number of samples for the synthetic dataset\n",
    "X, y = generate_data(num_samples)  # Generate the input-output pairs for the dataset\n",
    "\n",
    "X_mean = X.mean(axis = 0)  # Calculate the mean of the input data\n",
    "\n",
    "X -= X_mean  # Subtract the mean from the input data (centering)\n",
    "X /= np.var(X, axis = 0)  # Normalize the input data by dividing by its variance\n",
    "\n",
    "# Split the dataset into training and testing sets, with a 50% test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5)\n",
    "\n",
    "training_epoch = 50000  # Specify the number of training epochs\n",
    "\n",
    "# Initialize arrays to store the training and testing loss values for each epoch\n",
    "train_loss_list = np.zeros(training_epoch)\n",
    "test_loss_list = np.zeros(training_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(training_epoch):\n",
    "    # Perform a forward pass through the network using the training data\n",
    "    p = net.forward_pass(X_train)\n",
    "\n",
    "    # Calculate the training loss using the Mean Squared Error (MSE) loss function\n",
    "    train_loss = mse.forward_pass(p, y_train)\n",
    "    # Store the training loss for the current epoch\n",
    "    train_loss_list[i] = train_loss\n",
    "\n",
    "    # Compute the gradient of the MSE loss function\n",
    "    grad = mse.backward_pass()\n",
    "    # Perform a backward pass through the network to propagate the gradients\n",
    "    net.backward_pass(grad)\n",
    "    # Update the network parameters using the computed gradients and learning rate\n",
    "    net.update(0.1)\n",
    "\n",
    "    # Perform a forward pass through the network using the testing data\n",
    "    p = net.forward_pass(X_test)\n",
    "\n",
    "    # Calculate the testing loss using the Mean Squared Error (MSE) loss function\n",
    "    test_loss = mse.forward_pass(p, y_test)\n",
    "    # Store the testing loss for the current epoch\n",
    "    test_loss_list[i] = test_loss\n",
    "\n",
    "    # Print the current epoch, training loss, and testing loss\n",
    "    if i % 5000 == 0:\n",
    "        print(\"iteration %d: train_loss %f, test_loss %f\" % (i+1, train_loss, test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the training and testing loss to check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAFQCAYAAAA81jDRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAexAAAHsQEGxWGGAAA7B0lEQVR4nO3deVxU9f4/8NewDSAKI8qqCIYrIMp1QS1F7Io7ZSkuuF2veTP9qVzNHUFxuaVki2Zaimsu+XVNpQzUm9kupqKmsbjkDpiI7O/fHz2Y2zgsIwIzHl/Px4NHzee8P+d8PmemeXXmnDOjEhEBERGRgpgZewBERERVjeFGRESKw3AjIiLFYbgREZHiMNyISLHi4uKgUqkQFxdn7KFQDWO4kclLS0uDSqXCSy+9ZOyhENFTguFGRESKw3AjIiLFYbiR4uzatQsvvPACateujVq1aqFt27b4+OOP9eqKi4vx0UcfoW3bttBoNLC1tUWjRo0waNAgnD179rHrylNQUIC33noLvr6+sLGxgUajQc+ePXH06FGdun/84x9QqVQ4efJkqesZOXIkVCoVfv75Z532nTt3IigoCPb29rCxsUGbNm2wevVqvf5RUVFQqVQ4cuQI1qxZg1atWsHa2hqjRo2qcA7FxcVYs2YNOnToADs7O9jZ2aFTp074v//7P73aUaNGQaVS4bfffsOCBQvQuHFjWFtbo0WLFvjwww9LXf+tW7cwYcIENGrUCFZWVnB1dcXIkSORmppaav2FCxcwatQoNGzYEGq1Gm5ubujTpw++/PLLUusPHDiADh06wMbGBk5OThg/fjxycnL06rZv347nn38e9erVg42NDRo0aIB+/frh66+/rnAfkQkRIhOXmpoqACQ0NLTC2v/85z8CQJycnOSNN96QKVOmiKenpwCQ8ePH69ROnTpVAEirVq1k0qRJMm3aNBk8eLA4OTnJxo0bH7uuLEVFRdKnTx8BID4+PjJt2jT55z//KbVr1xYzMzP59NNPtbWHDx8WADJ16lS99eTk5Ejt2rWlRYsWpc7D09NTxo4dK5MmTZKWLVsKAJk8ebJO7bx58wSA9OzZU+zs7GTo0KHy5ptvSmxsbLlzKC4ulkGDBmnnMH78eBk/frw0atRIAMjy5ct16keOHCkApE+fPuLs7CwTJ06UyZMni7u7uwCQyMhInfqbN29qn6cePXrIjBkzZMCAAaJSqcTR0VHOnTunU5+QkCC1atUSc3NzCQ0NlZkzZ8qYMWOkRYsWMnLkSG3dunXrtK8dtVotgwYNkoiICPH19RUAEhYWprPeDz74QADIc889J2+88YZMnz5dhg8fLo0aNZIFCxaUu4/ItDDcyOQZGm4XL14Uc3NzadCggdy4cUPbfv/+fWnVqpUAkCNHjmjbNRqN/O1vf5PCwkKd9RQUFEhWVtZj15Wl5A22V69eUlBQoG0/d+6c2Nrair29vdy7d09E/gxCNzc3adCggRQVFemsZ+vWrQJAYmJitG2HDh0SADJgwADJzc3Vtufn50toaKgAkO+//17bXhJuderU0QuM8qxatUoAyMSJE3XGlZ2dLe3btxcrKyu5du2atr0k3FxdXXWei7t374qnp6eYm5vLr7/+qle/cOFCne2uX79eAEhQUJC2LScnR1xcXMTCwkKOHTumN9arV69q/71k31taWsqJEye07Q8fPpTmzZuLSqXSqW/Tpo24ubnJgwcPdNZZXFwsd+/eNWhfkWlguJHJMzTcoqKiBIC89957est27dolAGT06NHaNo1GI88//3yF2ze0rixBQUECQH755Re9ZZMmTRIAsn79em1bRESEAJDExESd2v79+4tKpZKUlBRtW79+/USlUsnNmzf11n369GkBIP/+97+1bSXhVtqRYXn8/PxEo9FIfn6+3rJ9+/YJAHn//fe1bSVhtXjxYr36FStWCACJjo4WEZHc3FyxtrYWV1dXycvL06tv3bq1AJD09HQREfn0008FgIwbN67CcZeE26hRo/SWlbxe9u7dq21r06aNeHl5lToOerpYVMtnnURGcOrUKQBA165d9ZYFBQXp1ABAWFgYVq1ahb/97W949dVXERQUhLZt28LS0lKnr6F15Y1Lo9HAz8+v1HG9++67OuMKDw9HbGwsNm/erB13RkYGDh06hE6dOsHLy0tb+91336FOnTpYuXKl3roLCgoAAOfPn9db1rZtW4PGDgA5OTk4c+YMPDw8sHDhQr3lt2/fLnM7zz//fJltJXO+cOECcnNzERgYCCsrK736rl27IikpCadOnYKHhwd+/PFHAECPHj0MnkObNm302tzd3QEAWVlZ2rawsDDMmDEDfn5+CAsLQ7du3RAYGAgbGxuDt0WmgeFGivHHH38AAJydnfWWOTg4QK1Wa2sA4L333oOnpyfWrVuHWbNmAQDs7e0xZswYLFy4ENbW1o9VV964mjRpUuoyFxcXnbEDf74Rt2jRAjt37sSKFStgZWWFzz77DPn5+Rg2bJhO/4yMDBQWFiI6OrrM7T948ECvzcnJqdwx/1VmZiZEBOnp6Y+9nfr16+u1lTw/JXMu73kD9PfRvXv3AABubm6GTgF16tTRa7Ow+PPtr6ioSNv25ptvom7duli5ciUWLFiABQsWwMbGBoMHD8ayZcug0WgM3iYZF6+WJMUoeQO7efOm3rJ79+4hLy9P503O0tIS06dPx/nz55Geno64uDg0b94csbGxmD59+mPXlTeu0sb017E++uY7bNgwZGZm4sCBAwCAzZs3w9LSEoMGDdJbt7u7O+TPUwyl/iUmJuptV6VSVTjuv24DADp37lzudtatW6fXt+Sorrw5l/e8lVbv4OAAAPj9998NnoOhVCoVxo4di5MnT+LGjRvYtm0bunTpgnXr1hl0RSmZDoYbKUbr1q0BAMeOHdNbVnLJfUnNozw8PDBy5EgkJibCzs4Oe/fufaK6R8eVmZmJM2fOGDyuoUOHQqVSYcuWLbhy5Qr++9//IiQkBI6Ojjp17du3x7Vr13DlypUKx1FZtWvXRvPmzXHmzBlkZ2c/Vt/SLp8vafP39wcANGvWDNbW1vjuu++Qn5+vV1/yfJbUt2vXDgDwxRdfPNZYHpezszMGDRqEAwcOoEmTJjh48CAKCwurdZtUdRhupBhDhw6Fubk53n77bZ0jhgcPHmDevHkAgBEjRgAA8vLycOLECb11ZGVlIS8vT/tRo6F15SnZ5syZM3U+Avv111+xevVq2NvbIzQ0VKePl5cXOnXqhH379mH16tUQEYSHh+ute+LEiQCAMWPGaD+u+6vU1FSkpaVVOMaKTJw4Effu3cMbb7yB3NxcveVnz57FrVu39Nrfe+89nSOyjIwMvP322zA3N8eQIUMAAGq1GmFhYfj999/xzjvv6PTfvHkzfv75ZwQFBcHDwwMA0L9/f7i5ueGTTz4pNTyf5Iju0fsOgT/POWZnZ8PS0hJmZnzLfFrwnBs9NX7++ecyPxrq0aMHhg4dikWLFmH69Onw8/PDwIEDYWVlhV27diE1NRXjx4/XXmzy8OFDdOrUCS1atEBAQAAaNmyIjIwM7NmzBwUFBYiIiHisuvKMGDECn332Gfbv3482bdqgV69eyMzMxLZt2/Dw4UNs3ry51HNCw4YNw/Hjx/Gf//wHtWvXRv/+/fVqevfujZkzZ2Lx4sXw9vZGSEgIGjRogFu3buHcuXP47rvvsGXLFnh6ehq+o0vx+uuv45tvvsGGDRtw5MgRBAcHw9nZGb///jtOnz6NpKQknDhxQu9cXkBAAPz9/TFo0CCYm5tjx44duHbtGiIjI3XOQ7711ls4evQoZsyYgcTERAQEBODXX3/Frl274OjoqHPjt7W1NT799FP07t0bQUFB6NevH1q0aIE7d+7gm2++Qdu2bSv9RcmhoaFwcHBAYGAgGjVqhJycHHz++ee4fv06Zs2axXB7mtT8BZpEj6fkVoDy/iZNmqSt37lzp3Tu3Flq1aolNjY2EhAQIKtXr9ZZZ35+vixZskT+/ve/i7u7u1hZWYmrq6uEhITIoUOHHruuIiXradmypajVarG3t5cePXro3Hf3qDt37oilpaUAkBEjRpS7/gMHDkjv3r3F0dFRLC0txc3NTbp06SJLly6V27dva+tKbgV49DYDQ23atEmCgoLEwcFBrKyspGHDhtKjRw9ZuXKlZGdna+tKbgW4dOmSzJ8/Xzw9PcXKykqaNWsmK1asKHXdN2/elDfeeEMaNmwolpaW4uzsLMOHD9e59eGvzp07J+Hh4eLi4qKdc9++feXw4cPampJbAdatW6fXv7RlK1eulL59+4qHh4eo1WqpX7++dOnSRbZu3Vqp/UXGoxIRMUqqEpFijRo1CuvXr0dqauoTHzUSVQaPsYmISHEYbkREpDgMNyIiUhyec6uEAQMG8DwCEVENSEtLK/VnlSrCWwEqwdPTE7GxscYeBhGR4hlyu01p+LEkEREpDsONiIgUh+FGRESKw3AjIiLFYbgREZHiMNyIiEhxeCsAET3zcnNzS/1hVape9evXN+hnoyqD4UZEz7Tc3FzcunUL7u7uMDc3N/ZwnhlFRUW4du0anJycqiXg+LEkET3Tbt++zWAzAnNzc7i7u1fbETPDjYieeQw246jO/c5wIyIixWG4ERGR4jDciIhIcRhuREQKoVKpKvx70p/riouLg0qlQlpaWpWMubrwVgAiIoU4ceKEzuOXX34Z/v7+iIqK0rap1eon2kafPn1w4sQJuLq6PtF6qhvDjYhIIQIDA3Ueq9Vq1KtXT6/9r4qKiiAisLAwLA7q16+P+vXrP9E4awI/liQieoaoVCrMnj0bS5YsgZeXF6ysrHD69Gnk5uZiypQp8PX1hZ2dHVxcXNCvXz+cP39ep39pH0t6enoiPDwcW7duRYsWLVCrVi20bdsWX3/9dQ3P7n945EZE9IyJi4tD48aNsXTpUtSqVQtubm7Iy8vD/fv3MWfOHLi6uiIjIwMrV65Ex44dce7cObi4uJS7zv/+97+4cOECFixYAGtra8ydOxd9+/ZFWloaHBwcamZif8FwIyJ6xMP8Ivx2O9vYw8Bz9e1gY1X1NzqLCL744gvY2NjotH/88cfafy8qKkJISAicnZ3x6aefYsqUKeWu848//kBSUhI0Gg0AwMXFBe3atcOBAwcwdOjQKp9DRRhuRESP+O12Nvp+8F9jDwP7J7wAX3f7Kl9vz5499YINALZv345ly5bhwoULuHfvnrb9woULFa6zY8eO2mADAD8/PwDA5cuXq2DEj4/hRkT0iOfq22H/hBeMPQw8V9+uWtZb2pWO+/btQ1hYGEaOHIl58+ahXr16MDMzQ+/evZGbm1vhOuvWravzuOSqTEP6VgeGGxHRI2yszKvliMlUqFQqvbatW7fC29sbcXFx2raCggJkZGTU4MiqDq+WJCIi5OTk6N0OsHHjRhQVFRlpRE+GR25ERISePXti9+7dmDJlCvr27Ysff/wR77//vlGudKwKDDciIsLYsWNx5coVrF27Fh999BHatWuHffv24eWXXzb20CqF4UZEpFClff+jiJRaa2ZmhpiYGMTExJS7jlGjRmHUqFEVbqe8bdUEnnMjIiLFYbgREZHiMNyIiEhxTDbcrly5gldffRX29vaoU6cOBgwYYPCd7rm5uZg2bRpcXV1hY2ODjh074tixY+X22bp1K1QqFRo0aFAVwyciIiMyyXDLyclBcHAwzp8/j/Xr12Pjxo24ePEiunXrhgcPHlTYf8yYMVizZg3mz5+P/fv3w9XVFSEhIUhKSiq1PisrC5MnT67wi0GJiOjpYJJXS65ZswYpKSm4cOECvL29AQCtWrVCkyZN8NFHHyEiIqLMvqdOncKWLVuwdu1ajB49GgDQtWtX+Pj4IDIyEnv37tXr8+abb8Lf3x+urq44fPhw9UyKiIhqjEkeue3duxeBgYHaYAMALy8vdO7cGXv27Kmwr6WlJcLCwrRtFhYWGDx4MOLj45GXl6dTf/z4cWzatAkrVqyo2kkQEZHRmGS4nT17Fr6+vnrtPj4+SE5OrrCvl5cXbG1t9frm5+fj0qVL2raCggK89tprmDZtmk6QEhHR080kwy0jI0PnpxNK1K1bF5mZmZXuW7K8xH/+8x/k5eVh5syZTzhiIiIyJSZ5zq0mXLp0CQsXLsSuXbtgbW1tUJ/4+HjEx8eXeTc+ERGZBpM8ctNoNKUeoZV1VGZoX+B/R3D/7//9PwQHByMwMBBZWVnIyspCfn4+RARZWVl4+PCh3jpCQkIQGxsLT0/PSsyKiKh6qVSqCv+q4v0rKSkJUVFRJv1zOCZ55Obj44OzZ8/qtScnJ6Nly5YV9t21axdycnJ0zrslJyfDyspKe24tOTkZ6enppYalRqPBpEmTsHz58iebCBFRDTpx4oTO45dffhn+/v6IiorStpX8iOiTSEpKQnR0NMLDw/V+pNRUmGS49e/fH1OnTkVKSgoaN24M4M8v5jx+/DiWLFlSbt9+/fph3rx52LFjB0aOHAkAKCwsxLZt29CjRw/tE7t161a9X4hdsmQJfvrpJ+zYsYM3cxPRUycwMFDnsVqtRr169fTanwUm+bHk2LFj4enpidDQUOzZswd79+5FaGgoGjZsiHHjxmnr0tPTYWFhgfnz52vb2rRpg7CwMEyePBkff/wxvvrqKwwePBipqamIjo7W1gUGBiIoKEjnz8XFBWq1GkFBQbx6kogUKTU1FcOGDUP9+vWhVqvRunVr7Nq1S6fm119/xcsvvwwnJydYW1vDw8MDAwcORGFhIeLi4rT3EDdp0kT7caepXYtgkuFWq1YtJCQkoGnTphg+fDiGDRsGLy8vJCQkwM7OTlsnIigqKkJxcbFO/3Xr1mH06NGYM2cO+vTpgytXruDQoUMICAio6akQEZmMK1euoEOHDjh16hTeeecd7N27FwEBAXjllVd0vuCiT58+uHbtGj788EPEx8djyZIlUKvVKC4uRp8+fTBnzhwAwI4dO3DixAmcOHECrq6uxppWqUzyY0kA8PDwwM6dO8ut8fT0LPX3gmxsbBAbG4vY2NjH2mZcXNxj1RORMkl+DnDnvLGHAdRrDpWVbcV1BoqKioKI4OjRo3B0dATw54VyV65cQWRkJPr37487d+7g0qVL2LNnD/r376/tO3ToUABA/fr18dxzzwEAWrdubbKfcplsuBERGc2d88Dqvxl7FMBrPwFuVfeJ06FDh9C7d2/Y29ujsLBQ2x4SEoJp06bhjz/+gKOjIxo3bowZM2bg5s2bCAoKQpMmTapsDDWF4UZE9Kh6zf8MFmOr17xKV3fr1i1s2LABGzZsKHX53bt3UadOHXz55ZeIiorCzJkzcffuXXh5eWHatGl4/fXXq3Q81YnhRkT0CJWVbZUeMZkKR0dHvPDCC5g+fXqpy93c3AAAjRs3xoYNGyAiOHXqFD744AOMHz8enp6e6NWrV00OudIYbkREz4iePXvixIkT8PHxgY2NTYX1KpUKrVu3RmxsLD755BOcOXMGvXr10t5SVdqXXZgKhhsR0TNi/vz5aN++Pbp06YIJEybA09MTmZmZOHPmDFJSUrB27Vr88ssvmDRpEsLCwuDt7Y2ioiLExcXBwsICwcHBAKD9Mo0VK1Zg5MiRsLS0RKtWrWBlZWXM6elguBERPSM8PDzw448/IioqCrNmzcLt27fh6OgIX19f7ZdeuLi4wMPDA7Gxsbh69Sqsra3h5+eH/fv3429/+/Mim5JvPVm9ejXWrFmD4uJipKammtRXEzLciIgUqrQbqxs0aICPP/64zD5OTk5Yv359heueN28e5s2b9yTDq1YmeRM3ERHRk2C4ERGR4jDciIhIcRhuRESkOAw3IiJSHIYbET3zSvsCdqp+1bnfGW5E9ExTq9Um/U0bSvbw4cMq+WXw0jDciOiZ5ujoiDt37qCgoMDYQ3mmFBQU4M6dO9qf3qlqvImbiJ5p5ubmqF+/Pm7duqX3w8dUfczMzFC/fn2Ym5tXy/oZbkT0zLOxsYG7u7uxh0FViB9LEhGR4jDciIhIcRhuRESkOAw3IiJSHIYbEREpDsONiIgUh+FGRESKw3AjIiLFYbgREZHiMNyIiEhxGG5ERKQ4DDciIlIchhsRESkOw42IiBSH4UZERIrDcCMiIsVhuBERkeIw3IiISHEYbkREpDgmG25XrlzBq6++Cnt7e9SpUwcDBgzA5cuXDeqbm5uLadOmwdXVFTY2NujYsSOOHTumU/Prr79i0qRJaNWqFezs7ODq6or+/fvj1KlT1TEdIiKqQSYZbjk5OQgODsb58+exfv16bNy4ERcvXkS3bt3w4MGDCvuPGTMGa9aswfz587F//364uroiJCQESUlJ2povvvgCiYmJGDlyJPbt24eVK1fi9u3bCAwMxE8//VSNsyMiomonJmj58uViZmYmFy9e1LalpKSIubm5LFu2rNy+SUlJAkDWrl2rbSsoKJCmTZtKv379tG23b9+W4uJinb5ZWVni4OAgw4cPL3cbU6ZMeZzpEBFRJVX2/dYkj9z27t2LwMBAeHt7a9u8vLzQuXNn7Nmzp8K+lpaWCAsL07ZZWFhg8ODBiI+PR15eHgCgXr16UKlUOn3t7e3RtGlTXLt2rQpnQ0RENc0kw+3s2bPw9fXVa/fx8UFycnKFfb28vGBra6vXNz8/H5cuXSqzb0ZGBs6cOYMWLVpUbuBERGQSTDLcMjIyoNFo9Nrr1q2LzMzMSvctWV6WiRMnQkQwefLkxxswERGZFAtjD8BULF68GFu2bMEnn3yi83HoX8XHxyM+Ph5paWk1OzgiInosJnnkptFoSj1CK+uozNC+wP+O4P5q1apVmDVrFmJiYvCPf/yjzHWHhIQgNjYWnp6eFcyAiIiMySTDzcfHB2fPntVrT05ORsuWLSvsm5qaipycHL2+VlZWekdlGzduxPjx4/Hvf/8bs2fPfvLBExGR0ZlkuPXv3x/ffvstUlJStG1paWk4fvw4+vfvX27ffv36oaCgADt27NC2FRYWYtu2bejRowfUarW2fdeuXRg9ejT++c9/YunSpVU/ESIiMgqTPOc2duxYfPDBBwgNDUVMTAxUKhXmzp2Lhg0bYty4cdq69PR0PPfcc4iMjERkZCQAoE2bNggLC8PkyZNRUFAALy8vfPjhh0hNTcXmzZu1fY8dO4YhQ4bA398fo0aNwrfffqtdplar0aZNm5qbMBERVSmTDLdatWohISEBU6ZMwfDhwyEi6N69O5YvXw47OzttnYigqKgIxcXFOv3XrVuH2bNnY86cOcjKyoK/vz8OHTqEgIAAbU1CQgLy8vLw888/o3Pnzjr9GzVqxItGiIieYioREWMP4mkTERGB2NhYYw+DiEjxKvt+a5Ln3IiIiJ4Ew42IiBSH4UZERIrDcCMiIsVhuBERkeIw3IiISHEYbkREpDgMNyIiUhyGGxERKQ7DjYiIFIfhRkREisNwIyIixWG4ERGR4jDciIhIcRhuRESkOAw3IiJSHIYbEREpDsONiIgUh+FGRESKw3AjIiLFYbgREZHiMNyIiEhxGG5ERKQ4DDciIlIchhsRESkOw42IiBSH4UZERIrzROF29+7dqhoHERFRlTEo3NasWYO3335b+/j06dNo0KABnJyc0LZtW9y4caPaBkhERPS4DAq3999/HzY2NtrHERERcHBwwPLly3Hv3j1ERkZW2wCJiIgel4UhRenp6WjevDkA4N69ezh69Ch2796N3r17w9HRETNnzqzWQRIRET0Og47ciouLYWb2Z+nXX38NlUqFoKAgAEDDhg1x69atahsgERHR4zIo3Jo0aYLPP/8cALB161Z06tQJtra2AIDff/8ddevWrb4REhERPSaDPpacOnUqhg8fjvXr1yMzMxM7duzQLktMTESrVq2qbYBERESPy6BwGzp0KDw8PPDdd9+hXbt26NKli3aZs7Mz+vfvX20DJCIielwGhRsAPP/883j++ef12qOjo6t0QERERE/KoHNu33zzDfbv3699fPfuXQwZMgR+fn6YOnUqioqKqnxgV65cwauvvgp7e3vUqVMHAwYMwOXLlw3qm5ubi2nTpsHV1RU2Njbo2LEjjh07pldXXFyMxYsXw9PTE9bW1vD398fOnTureipERFTDDAq3GTNm4KefftI+njZtGg4cOICmTZviww8/xKJFi6p0UDk5OQgODsb58+exfv16bNy4ERcvXkS3bt3w4MGDCvuPGTMGa9aswfz587F//364uroiJCQESUlJOnVz585FVFQUJkyYgIMHDyIwMBADBw7EgQMHqnQ+RERUw8QA9erVk/3794uISH5+vtSuXVs++eQTERF55513pHnz5oasxmDLly8XMzMzuXjxorYtJSVFzM3NZdmyZeX2TUpKEgCydu1abVtBQYE0bdpU+vXrp227efOmWFlZSWRkpE7/4OBg8fPzK3cbU6ZMeZzpEBFRJVX2/dagI7fs7GzUqVMHAPD999/jwYMH6Nu3LwAgICDA4I8LDbV3714EBgbC29tb2+bl5YXOnTtjz549Ffa1tLREWFiYts3CwgKDBw9GfHw88vLyAADx8fHIz89HeHi4Tv/w8HCcPn0aqampVTgjIiKqSQZdUOLu7o5Tp07hhRdewMGDB+Hr6wsnJycAQGZmpvaet6py9uxZhIaG6rX7+Pjo3IZQVl8vLy+9Mfn4+CA/Px+XLl2Cj48Pzp49C7VarROgJXUAkJycDC8vryecia683Bw8+COjStdJRGTq7Ou6wNzC4OsXq4RBWxsyZAhmzZqFI0eO4MCBAzpXSP78889o0qRJlQ4qIyMDGo1Gr71u3brIzMysdN+S5SX/dHBwgEqlKreuKp1N3IQ2342r8vUSEZmy22Mvwsndu+LCKmRQuEVFRcHa2hrffvstZsyYgSlTpmiXnTp1CgMHDqy2AZqS+Ph4xMfHIy0trVL9G/q/iCSLuCodExGRqWuhcanxbRoUbubm5pg9e3apy3bv3l2V4wEAaDSaUo/Qyjoqe7Rvenp6qX2B/x2ZaTQaZGVlQUR0jt4erfurkJAQhISEICIiwvDJ/EV9t8ao79a4Un2JiMhwj/Uh6JkzZ3D06FFkZGSgbt26CAoK0p6jqkol58QelZycjJYtW1bYd9euXcjJydE575acnAwrKyvtOTYfHx/k5eXht99+0znvlpycDAAVboeIiEyXQVdLFhYWIjw8HP7+/pg4cSLmzZuHiRMnolWrVhg+fHiV38Tdv39/fPvtt0hJSdG2paWl4fjx4xV+1Ve/fv1QUFCgc+FJYWEhtm3bhh49ekCtVgMAevbsCUtLS2zevFmn/6ZNm+Dr61vlF5MQEVENMuR+gTlz5oilpaXExMRIWlqa5ObmSlpamsTExIilpaXevWJPKjs7W5577jnx9fWV3bt3y549e6RVq1bi5eUl9+/f19alpaWJubm5REdH6/QPCwsTBwcHWbNmjRw+fFheeeUVUavV8tNPP+nUTZ8+XdRqtSxbtkwSExPlX//6l6hUKtm3b1+54+N9bkRENaOy77cGhZunp6degJSIjo4WT0/PSm28POnp6TJgwACpXbu22NnZSWhoqKSmpurUpKamCgCZN2+eTntOTo5MmTJFnJ2dRa1WS/v27SUxMVFvG4WFhbJgwQLx8PAQKysr8fPzkx07dlQ4NoYbEVHNqOz7rUpEpKKjO7Vajc8//xwvvvii3rLDhw+jT58+2pujnwURERGIjY019jCIiBSvsu+3Bp1zc3Nzw/Hjx0td9s0338DNze2xN0xERFRdDLpactiwYVi4cCHMzMwwbNgwuLq64saNG9i6dSsWLlyI6dOnV/c4iYiIDGbwTdwpKSmYN28eoqKitO0igqFDhyIyMrK6xkdERPTYDAo3CwsLbNmyBbNnz8axY8e097l16dIF169fR0BAAH755ZfqHisREZFBHusmbh8fH72bts+fP1/qDddERETGYtAFJURERE8ThhsRESkOw42IiBSnzHNuf/1ex/LcuHGjygZDRERUFcoMN29vb70f8iyNPPKTMURERMZWZritW7euJsdBRERUZcoMt5EjR9bkOIiIiKoMLyghIiLFYbgREZHiMNyIiEhxGG5ERKQ4DDciIlIchhsRESkOw42IiBSH4UZERIrDcCMiIsVhuBERkeIw3IiISHEYbkREpDgMNyIiUhyGGxERKQ7DjYiIFIfhRkREisNwIyIixWG4ERGR4jDciIhIcRhuRESkOAw3IiJSHIYbEREpDsONiIgUh+FGRESKY7LhVlxcjMWLF8PT0xPW1tbw9/fHzp07De6/e/dutGnTBtbW1mjUqBFiYmJQVFSkXV5UVISlS5ciODgYzs7OqF27NgICAvDJJ5+guLi4OqZEREQ1xGTDbe7cuYiKisKECRNw8OBBBAYGYuDAgThw4ECFfePj4/HKK6+gXbt2OHjwICZNmoSYmBjMmjVLW/Pw4UPExMTA19cXq1evxu7du9GtWzeMHTsW06dPr86pERFRdRMTdPPmTbGyspLIyEid9uDgYPHz86uwf+vWraVLly46bdHR0WJpaSnXr18XEZHCwkK5e/euXt/Ro0eLWq2WnJycMtc/ZcoUQ6ZBRERPqLLvtyZ55BYfH4/8/HyEh4frtIeHh+P06dNITU0ts++VK1eQlJSk13f48OEoKCjAwYMHAQDm5uaoW7euXv927dohLy8Pd+7cqYKZEBGRMZhkuJ09exZqtRre3t467T4+PgCA5OTkcvsCgK+vr067l5cXbG1ty+0LAEePHoWDgwNcXV0rM3QiIjIBJhluGRkZcHBwgEql0mkvOdLKyMgoty8AaDQavWUajabcvvHx8di+fTumTp0KCwuLygydiIhMQI2E2+HDh6FSqSr8CwoKqonhlCo5ORlDhgxBt27dyrygJD4+HhEREUhLS6vZwRER0WOpkcOTTp064dy5cxXW2draAvjzCCsrKwsionP0VnLUVdq5shIlR2yZmZl6yzIzM0vtm5KSgr///e/w8vLCrl27yjxqCwkJQUhICCIiIiqcCxERGU+NhJutrS2aN29ucL2Pjw/y8vLw22+/6Zx3Kzlf1rJly3L7An+ee+vYsaO2PS0tDTk5OXp9r169iu7du6NOnTqIj49HnTp1DB4nERGZJpM859azZ09YWlpi8+bNOu2bNm2Cr68vvLy8yuzr4eEBf3//UvtaWlqiV69e2rbbt2/jxRdfBAB8+eWXqFevXhXOgoiIjMUkr5pwcnJCREQEFi9erP3mkG3btiEhIQF79+7Vqe3evTvS09Nx6dIlbduiRYvQt29fjBs3DkOGDMHJkycRExODSZMmwcXFBcCfN3GHhIQgLS0Na9euxdWrV3H16lXtOlq2bMmjOCKip5RJhhsALFy4EHZ2dnj33Xdx48YNNGvWDNu3b0ffvn116oqKilBYWKjT1rt3b3z22WeIjo5GXFwcnJ2dMWvWLMyePVtbc/PmTZw8eRIAMGzYML3tJyYmGvUCFyIiqjyViIixB/G0iYiIQGxsrLGHQUSkeJV9vzXJc25ERERPguFGRESKw3AjIiLFYbgREZHiMNyIiEhxGG5ERKQ4DDciIlIchhsRESkOw42IiBSH4UZERIrDcCMiIsVhuBERkeIw3IiISHEYbkREpDgMNyIiUhyGGxERKQ7DjYiIFIfhRkREisNwIyIixWG4ERGR4jDciIhIcRhuRESkOAw3IiJSHIYbEREpDsONiIgUh+FGRESKw3AjIiLFYbgREZHiMNyIiEhxGG5ERKQ4DDciIlIchhsRESkOw42IiBSH4UZERIrDcCMiIsVhuBERkeKYbLgVFxdj8eLF8PT0hLW1Nfz9/bFz506D++/evRtt2rSBtbU1GjVqhJiYGBQVFZVZn5WVBVdXV6hUKhw+fLgqpkBEREZisuE2d+5cREVFYcKECTh48CACAwMxcOBAHDhwoMK+8fHxeOWVV9CuXTscPHgQkyZNQkxMDGbNmlVmn+nTp1fl8ImIyJjEBN28eVOsrKwkMjJSpz04OFj8/Pwq7N+6dWvp0qWLTlt0dLRYWlrK9evX9eq//vprsbW1lU8++UQAyJdfflnu+qdMmWLALIiI6ElV9v3WJI/c4uPjkZ+fj/DwcJ328PBwnD59GqmpqWX2vXLlCpKSkvT6Dh8+HAUFBTh48KBOe0FBAcaNG4cZM2agcePGVTcJIiIyGpMMt7Nnz0KtVsPb21un3cfHBwCQnJxcbl8A8PX11Wn38vKCra2tXt+33noL+fn5ePPNN6ti6EREZAIsjD2A0mRkZMDBwQEqlUqnvW7dutrl5fUFAI1Go7dMo9Ho9L106RJiYmKwb98+qNXqqhg6ERGZgBo5cjt8+DBUKlWFf0FBQTUxHK3XX38doaGhePHFFw2qj4+PR0REBNLS0qp3YERE9ERq5MitU6dOOHfuXIV1tra2AP48wsrKyoKI6By9lRx1lRzBlabkiC0zM1NvWWZmprbv9u3b8c033+CHH35AVlYWACA7OxsA8ODBA9y7dw/29vY6/UNCQhASEoKIiIgK50JERMZTI+Fma2uL5s2bG1zv4+ODvLw8/Pbbbzrn3UrOl7Vs2bLcvsCf5946duyobU9LS0NOTo62b3JyMnJycrT1f/XSSy/B3t5eG3pERPR0MckLSnr27AlLS0ts3rxZp33Tpk3w9fWFl5dXmX09PDzg7+9fal9LS0v06tULADBq1CgkJibq/L3zzjsAgKVLl2L//v1VPCsiIqopJnlBiZOTEyIiIrB48WLUrl0bAQEB2LZtGxISErB3716d2u7duyM9PR2XLl3Sti1atAh9+/bFuHHjMGTIEJw8eRIxMTGYNGkSXFxcAACenp7w9PQsdfv+/v54/vnnq21+RERUvUwy3ABg4cKFsLOzw7vvvosbN26gWbNm2L59O/r27atTV1RUhMLCQp223r1747PPPkN0dDTi4uLg7OyMWbNmYfbs2TU5BSIiMhKViIixB/G0iYiIQGxsrLGHQUSkeJV9vzXJc25ERERPguFGRESKw3AjIiLFYbgREZHiMNyIiEhxGG5ERKQ4DDciIlIchhsRESkOw42IiBSH4UZERIrDcCMiIsVhuBERkeIw3IiISHEYbkREpDgMNyIiUhyGGxERKQ7DjYiIFIfhRkREisNwIyIixWG4ERGR4jDciIhIcRhuRESkOAw3IiJSHIYbEREpDsONiIgUh+FGRESKw3AjIiLFYbgREZHiMNyIiEhxLIw9gKdRWloaIiIiKt3X09Ozagf0FHmW5/8szx14tufPuXs+Uf9KEapRU6ZMMfYQjOpZnv+zPHeRZ3v+nHvN48eSNSwkJMTYQzCqZ3n+z/LcgWd7/px7zVOJiBhly0RERNWER25ERKQ4DDciIlIchlsNuHLlCl599VXY29ujTp06GDBgAC5fvmzsYZXq6tWrmDhxIjp27AhbW1uoVKpSr1bKzc3FtGnT4OrqChsbG3Ts2BHHjh3TqysuLsbixYvh6ekJa2tr+Pv7Y+fOnaVue82aNWjevDnUajWaNWuGVatWlVq3e/dutGnTBtbW1mjUqBFiYmJQVFT0RPMGgM8++wyvvPIKGjVqBBsbGzRr1gwzZ87E/fv3deoyMzPxz3/+E/Xq1UOtWrXw4osv4vTp03rrM+Y+qoz4+HgEBwfDxcUFarUaDRo0wKBBg5CcnKxTZ+jr2Zj76Un17NkTKpUKc+bMMZk5Vddzf+TIEahUKr0/BweHp3vuRrmM5Rny4MED8fb2Fh8fH9m1a5fs3r1bfH19pXHjxpKdnW3s4elJTEwUJycn6dWrl/To0UMASGpqql7d0KFDxd7eXlavXi2HDx+Wl19+WaytreXkyZM6dbNmzRIrKyt5++23JSEhQV577TVRqVTy+eef69StXr1aVCqVzJo1SxISEmT27NmiUqlk5cqVOnWHDh0SMzMzGTt2rCQkJMiyZctErVbLm2+++cRz79ChgwwcOFA2bdokR44ckXfeeUfs7e2lQ4cOUlRUJCIixcXF0rlzZ3F3d5ctW7bIwYMHpUuXLuLo6ChXrlwxiX1UWVu2bJGpU6fKjh075MiRI7JhwwZp2bKl1K5dW9LS0kTE8NezMfdTVewHFxcXASCzZ882iTlV53OfmJgoAOS9996TEydOaP9++OGHp3ruDLdqtnz5cjEzM5OLFy9q21JSUsTc3FyWLVtmxJGVruRNXERkzZo1pYZbUlKSAJC1a9dq2woKCqRp06bSr18/bdvNmzfFyspKIiMjdfoHBweLn5+fTt/69evLiBEjdOpGjx4tjo6Okp+fr21r3bq1dOnSRacuOjpaLC0t5fr1648/4b+4deuWXtv69esFgHz11VciIrJ7924BIAkJCdqarKws0Wg0MnHiRG2bMfdRVTp//rwAkKVLl4qI4a9nY+2nJ5WRkSHOzs6yZcsWvXBT6nNfEm5ffvllmTVP49wZbtUsODhYOnXqpNfepUsXvTdpU1NWuM2fP18sLS3lwYMHOu2RkZFiZWUlubm5IiKyYcMGASC//vqrTt3atWsFgKSkpIiIyLFjxwSAfPHFFzp1CQkJOv9BXb58WQDI6tWrdepSUlL0/oOqKsnJyQJANmzYICIi//jHP8TNzU2vbsSIEeLh4aF9bKx9VNVu374tAGT58uUiYvjr2Vj76UmNHTtWunfvLiKiF25Kfe4NCbence4851bNzp49C19fX712Hx8fvXMZT4uzZ8/Cy8sLtra2Ou0+Pj7Iz8/HpUuXtHVqtRre3t56dQC08z979iwA6O0nQ+tKxlId+/Po0aMAgBYtWmjHUNbzefnyZWRnZ2vrjLGPqkJRURHy8/Nx8eJFjBs3Di4uLhgyZIh2HIa8no21n57E119/jQ0bNmDFihWlLlf6cz9s2DCYm5vD0dERQ4cO1TmP+jTOneFWzTIyMqDRaPTa69ati8zMTCOM6MmVN6eS5SX/dHBwgEqlqrAOgN46Da0raStZXlWuXbuGyMhIvPjii2jbtq12DOXNveQ5NdY+qgodOnSAWq1G06ZN8csvvyAhIQFOTk7a7RjyejbWfqqs/Px8jBs3DlOnTkWzZs1KrVHqc29vb49///vf+Pjjj5GQkIC5c+fi8OHD6NixI27dumXQnExx7vxuSaJSZGdnIzQ0FBYWFli3bp2xh1OjNm7ciD/++AMpKSlYunQp/v73v+Prr79W9HcjvvXWW3j48CFmz55t7KHUuDZt2qBNmzbax127dkWXLl3Qvn17vPfee4iJiTHi6CqPR27VTKPRlHqEVtb/4TwNypsT8L//q9JoNMjKyoI88iU4pdUB0FunoXUlbSV1T+rhw4fo168fUlJSEB8fjwYNGmiXVTT3kjEaax9VhRYtWqBDhw4YMmQIvvrqK2RnZ2PJkiXacRjyejbWfqqMy5cvY+HChViwYAHy8vKQlZWFrKwsANA+Lioqeiae+xIBAQFo2rQpfvjhB+0Ynra5M9yqmY+Pj/Zz479KTk5Gy5YtjTCiJ+fj44PU1FTk5OTotCcnJ8PKykr7ObqPjw/y8vLw22+/6dUB0M6/5PPzR/eToXVpaWnIycmpkv1ZUFCAV199FT/++CMOHDgAPz8/neXlPZ8eHh6ws7PT1hljH1U1BwcHeHt7a8+VGPp6NtZ+qoyUlBTk5uYiPDwcGo1G+wcAS5cuhUajwenTp5+55x6A9mPDp3LuZV5qQlXinXfeEXNzc/ntt9+0bampqWJhYaG9vNpUlXW15M8//ywAJC4uTttWUFAgzZs3l759+2rbbt68KZaWlhIVFaXTv3v37uLr66t9nJ+fL/Xq1ZNRo0bp1I0ZM0bq1q0reXl52jZ/f38JCgrSqVuwYEGV3ApQVFQkAwcOFGtrazl8+HCpNbt27RIAcuTIEW3bvXv3pG7dujJhwgRtmzH3UVW6ceOG1KpVS1577TURMfz1bKz9VBmZmZmSmJio9wdAwsPDJTExUe7fv/9MPfc//PCDmJmZydy5c0Xk6XzdM9yqWXZ2tjz33HPi6+sru3fvlj179kirVq3Ey8tL7t+/b+zhlWrHjh2yY8cO+de//iUAZOXKldobe0uEhYWJg4ODrFmzRg4fPiyvvPKKqNVq+emnn3TWNX36dFGr1bJs2TJJTEyUf/3rX6JSqWTfvn06dR9++KGoVCqZPXu2JCYmyty5c0WlUskHH3ygU/f555+LSqWS1157TRITEyU2NlbUarVMnTr1ieddMt/Zs2fr3Mx64sQJ7Y2qRUVF0rFjR2nQoIF8+umncujQIenatatoNBq5fPmyzvqMtY8q66WXXpL58+fL7t27JSEhQVatWiXNmjUTe3t7uXDhgogY/no25n6qKnjkVgClPvdDhw6V2bNny86dO+Wrr76SpUuXiqOjozRs2FBu37791M6d4VYD0tPTZcCAAVK7dm2xs7OT0NDQUr/1w1QAKPWva9eu2pqcnByZMmWKODs7i1qtlvbt20tiYqLeugoLC2XBggXi4eEhVlZW4ufnJzt27Ch1u6tWrZImTZqIlZWVeHt7y4oVK0qt27lzp7Rq1UqsrKykYcOGEh0dLYWFhU8870aNGpU593nz5mnr7t69K6NHjxaNRiM2NjYSHBwsSUlJeusz5j6qjCVLlkhAQIDY29uLjY2NNG3aVF577TW916qhr2dj7qeq8Gi4iSjzuV+0aJH4+flJnTp1xMLCQho0aCBjx46V33///ameO3/yhoiIFIcXlBARkeIw3IiISHEYbkREpDgMNyIiUhyGG5ERxcXFlfpDkaX9WGRNGjVqlM43sxA9bfjdkkQmYMeOHXphYmHB/zyJKov/9RCZgNatW+v9/AcRVR4/liQycSUfXR47dgwvvfQS7Ozs4OjoiDfeeAMPHz7Uqb1+/TpGjBiBevXqQa1Wo1WrVti0aZPeOlNTUzF8+HC4uLhArVajcePGmDRpkl7dyZMn8cILL8DW1hZNmjTBqlWrdJbfuHEDI0eOhJubG9RqNVxdXdG3b1/tT6UQGQuP3IhMQFFREQoLC3XazMzMYGb2v///DA8Px6BBgzB+/Hh8//33mD9/Ph48eIC4uDgAwIMHD9C1a1dkZmZi0aJFaNiwITZt2oThw4cjJycHr732GoA/g619+/awtbXF/Pnz0aRJE1y+fBlffPGFzvb/+OMPDB06FJMnT0ZkZCTWrVuH119/Hc2aNUO3bt0AAMOHD0d6ejrefvttNGzYEDdv3sRXX32l98W5RDXOkK9nIaLqsW7dujK/8qtPnz46NePGjdPpGxMTI2ZmZtrvfXz//fcFgN5XHXXv3l3q16+v/Yqy4cOHS61ateTatWtljmvkyJECQBISErRtubm5UrduXRk7dqy2rVatWvLuu+8+0T4gqg48ciMyAbt27dK7oOTRqyUHDRqk83jw4MGYM2cOvv/+ezRt2hTHjh2Du7s7goKCdOrCw8MxevRoJCcnw8/PD1988QX69u0LNze3csdka2urPUIDoP117suXL2vb2rVrh7fffhsiguDgYPj6+ur9ujKRMfCcG5EJ8PX1Rdu2bXX+Hr3AxNnZudTH165dA/DnDzi6urrqrdvFxUW7HADu3r1r0GX+pf2YrlqtRm5urvbxtm3b0L9/f7z11lto1aoV3N3dMX/+fBQXF1e4fqLqxHAjekrcvHmz1Mfu7u4A/vxV4hs3buj1K2kr+dXievXqaQPxSTk5OWHFihW4du0azp8/j1GjRmHevHn46KOPqmT9RJXFcCN6Smzfvl3n8datW2FmZoYOHToAALp27YqrV6/i+PHjOnVbtmyBk5OT9leLe/Togf379+P69etVOr5mzZph0aJF0Gg0OHPmTJWum+hx8ZwbkQlISkrCnTt39Nrbtm2r/fcDBw5g2rRp6NGjB77//ntER0djxIgRaNKkCYA/v1Xk3XffxYABA7Bw4UI0aNAAmzdvxpdffomPPvoI5ubmAIDo6GgcOHAAnTp1wqxZs+Dt7Y1r167h0KFDpd42UJZ79+7hxRdfxLBhw9C8eXNYWlpiz549yMzMRI8ePZ5wjxA9GYYbkQkYOHBgqe23b9/W/vumTZuwbNkyfPjhh7CyssLYsWOxdOlS7fJatWrh6NGjePPNNzFjxgzcv38fzZo1w8aNGxEeHq6t8/T0xLfffos5c+Zg5syZyM7Ohru7O0JDQx9rzNbW1ggICMCaNWuQnp4OMzMzNGvWDJs3b37sdRFVNf5YKZGJi4uLw+jRo3Hx4kV+iwmRgXjOjYiIFIfhRkREisOPJYmISHF45EZERIrDcCMiIsVhuBERkeIw3IiISHH+Pzj+cZRAHdBmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 450x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Loss over epochs\")\n",
    "plt.plot(train_loss_list, label=\"Train\")\n",
    "plt.plot(test_loss_list, label=\"Test\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the results by plotting the ground truth Sigmoid function and the model's predictions on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 450x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate linspace for X_pred\n",
    "X_pred = np.linspace(X_test.min(), X_test.max(), 1000)[:, None]\n",
    "\n",
    "# Forecast the y values for X_pred using the trained network\n",
    "y_pred = net.forward_pass(X_pred)\n",
    "\n",
    "# Plot the ground truth and the best fit curve\n",
    "plt.scatter(X_test, y_test, label='Ground Truth', color='blue', alpha=0.5)\n",
    "plt.plot(X_pred, y_pred, label='Best Fit', color='red', lw=3)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Ground Truth vs Best Fit Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, you have learned how to build backpropagation from scratch using only NumPy. This careful dissection should provide you with a better understanding of how popular machine learning packages like PyTorch and TensorFlow work.\n",
    "\n",
    "While these libraries offer powerful and optimized tools to build, train, and deploy complex neural networks, knowing the underlying principles and methods can be crucial in developing a solid foundation in machine learning. By learning how to implement backpropagation and neural networks from scratch, you can better appreciate the intricacies and challenges associated with training models, and also develop a more intuitive understanding of various aspects of the training process.\n",
    "\n",
    "Remember, it's not just about using powerful tools; it's about understanding the fundamental concepts that make them so powerful. Now that you have a deeper insight into backpropagation and neural networks, you are better equipped to leverage the full potential of machine learning packages like PyTorch and TensorFlow in your future projects. Happy learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
